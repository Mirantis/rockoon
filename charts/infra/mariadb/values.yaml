# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# Default values for mariadb.
# This is a YAML-formatted file.
# Declare name/value pairs to be passed into your templates.
# name: value

---
release_group: null

images:
  tags:
    # 10.2.31
    mariadb: openstackhelm/mariadb@sha256:5f05ce5dce71c835c6361a05705da5cce31114934689ec87dfa48b8f8c600f70
    ingress: quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.9.0
    error_pages: gcr.io/google_containers/defaultbackend:1.4
    prometheus_create_mysql_user: docker.io/mariadb:10.2.31
    prometheus_mysql_exporter: docker.io/prom/mysqld-exporter:v0.10.0
    prometheus_mysql_exporter_helm_tests: docker.io/openstackhelm/heat:newton-ubuntu_xenial
    dep_check: quay.io/airshipit/kubernetes-entrypoint:v1.0.0
    image_repo_sync: docker.io/docker:17.07.0
    mariadb_backup: docker.io/openstackhelm/mariadb:ubuntu_xenial-20191031
    mariadb_phy_backup: docker.io/openstackhelm/mariadb:10.2.18
    mariadb_phy_restore: docker.io/openstackhelm/mariadb:10.2.18
    scripted_test: docker.io/openstackhelm/mariadb:ubuntu_xenial-20191031
    mariadb_controller: docker.io/openstackhelm/mariadb:10.2.18
  pull_policy: "IfNotPresent"
  local_registry:
    active: false
    exclude:
      - dep_check
      - image_repo_sync

labels:
  server:
    node_selector_key: openstack-control-plane
    node_selector_value: enabled
  ingress:
    node_selector_key: openstack-control-plane
    node_selector_value: enabled
  prometheus_mysql_exporter:
    node_selector_key: openstack-control-plane
    node_selector_value: enabled
  error_server:
    node_selector_key: openstack-control-plane
    node_selector_value: enabled
  job:
    node_selector_key: openstack-control-plane
    node_selector_value: enabled
  test:
    node_selector_key: openstack-control-plane
    node_selector_value: enabled
  controller:
    node_selector_key: openstack-control-plane
    node_selector_value: enabled
pod:
  probes:
    server:
      mariadb:
        readiness:
          enabled: true
          disk_usage_percent: 99
          params:
            initialDelaySeconds: 30
            periodSeconds: 30
            timeoutSeconds: 15
        liveness:
          enabled: true
          params:
            initialDelaySeconds: 30
            periodSeconds: 180
            timeoutSeconds: 15
            # NOTE(vsaienko) the periodSeconds * failureThreshold should be higher than master election
            # The election mechanism migh pause for cluster_leader_ttl=120 defined in start.py
            failureThreshold: 3
      mariadb_exporter:
        readiness:
          enabled: true
          params:
            initialDelaySeconds: 5
            periodSeconds: 60
            timeoutSeconds: 10
        liveness:
          enabled: true
          params:
            initialDelaySeconds: 15
            periodSeconds: 60
            timeoutSeconds: 10
    controller:
      controller:
        readiness:
          enabled: true
          params:
            initialDelaySeconds: 5
            periodSeconds: 20
            timeoutSeconds: 5
        liveness:
          enabled: true
          params:
            initialDelaySeconds: 15
            periodSeconds: 60
            timeoutSeconds: 5
  security_context:
    server:
      pod:
        runAsUser: 999
        runAsNonRoot: true
      container:
        perms:
          runAsUser: 0
          runAsNonRoot: false
          readOnlyRootFilesystem: true
          allowPrivilegeEscalation: false
          # the unprivileged user mariadb-phy-restore scale mariadb-server STS
          # during DB restore process. In case of "capabilities:add" exists
          # scaling process will fail, so we have to drop unnecessary capabilities
          # individually and not use drop:ALL statement
          capabilities:
            drop:
            - FOWNER
            - FSETID
            - KILL
            - SETGID
            - SETUID
            - SETPCAP
            - SETFCAP
            - NET_BIND_SERVICE
            - SYS_CHROOT
            - MKNOD
            - AUDIT_WRITE
            - NET_RAW
        mariadb:
          runAsUser: 999
          allowPrivilegeEscalation: false
          readOnlyRootFilesystem: true
          capabilities:
            drop:
              - ALL
        exporter:
          runAsUser: 65534
          allowPrivilegeEscalation: false
          readOnlyRootFilesystem: true
          capabilities:
            drop:
              - ALL
    ingress:
      pod:
        runAsUser: 65534
      container:
        server:
          runAsUser: 0
          readOnlyRootFilesystem: false
    error_pages:
      pod:
        runAsUser: 65534
      container:
        server:
          allowPrivilegeEscalation: false
          readOnlyRootFilesystem: true
    controller:
      pod:
        runAsUser: 65534
        runAsNonRoot: true
      container:
        controller:
          allowPrivilegeEscalation: false
          readOnlyRootFilesystem: true
          capabilities:
            drop:
              - ALL
    prometheus_create_mysql_user:
      pod:
        runAsUser: 65534
        runAsNonRoot: true
      container:
        main:
          allowPrivilegeEscalation: false
          readOnlyRootFilesystem: true
          capabilities:
            drop:
              - ALL
    cluster_wait:
      pod:
        runAsUser: 65534
        runAsNonRoot: true
      container:
        mariadb_cluster_wait:
          allowPrivilegeEscalation: false
          capabilities:
            drop:
              - ALL
    mariadb_phy_backup:
      pod:
        runAsUser: 999
        runAsGroup: 999
        runAsNonRoot: true
      container:
        perms:
          runAsUser: 0
          readOnlyRootFilesystem: true
          runAsNonRoot: false
          allowPrivilegeEscalation: false
          capabilities:
            drop:
              - ALL
            add:
              - CHOWN
              - DAC_READ_SEARCH
        phy_backup:
          allowPrivilegeEscalation: false
          readOnlyRootFilesystem: true
          capabilities:
            drop:
              - ALL
    mariadb_phy_restore:
      pod:
        runAsUser: 999
        runAsGroup: 999
        runAsNonRoot: true
      container:
        perms:
          runAsUser: 0
          readOnlyRootFilesystem: true
          runAsNonRoot: false
          allowPrivilegeEscalation: false
          capabilities:
            drop:
              - ALL
            add:
              - CHOWN
              - DAC_READ_SEARCH
        phy_restore:
          allowPrivilegeEscalation: false
          readOnlyRootFilesystem: true
          capabilities:
            drop:
              - ALL
    tests:
      pod:
        runAsUser: 999
      container:
        test:
          runAsUser: 999
          readOnlyRootFilesystem: true
  env:
    mariadb_controller:
      MARIADB_CONTROLLER_DEBUG: 0
      MARIADB_CONTROLLER_CHECK_PODS_DELAY: 10
      MARIADB_CONTROLLER_PYKUBE_REQUEST_TIMEOUT: 60
  affinity:
    anti:
      type:
        default: preferredDuringSchedulingIgnoredDuringExecution
        controller: requiredDuringSchedulingIgnoredDuringExecution
      topologyKey:
        default: kubernetes.io/hostname
      weight:
        default: 10
  replicas:
    server: 3
    ingress: 2
    error_page: 1
    prometheus_mysql_exporter: 1
    controller: 2
  lifecycle:
    upgrades:
      deployments:
        revision_history: 3
        pod_replacement_strategy: RollingUpdate
        rolling_update:
          max_unavailable: 1
          max_surge: 3
    termination_grace_period:
      prometheus_mysql_exporter:
        timeout: 30
      error_pages:
        timeout: 10
      server:
        timeout: 600
    disruption_budget:
      mariadb:
        min_available: 0
  resources:
    enabled: false
    prometheus_mysql_exporter:
      limits:
        memory: "1024Mi"
        cpu: "2000m"
      requests:
        memory: "128Mi"
        cpu: "500m"
    server:
      requests:
        memory: "128Mi"
        cpu: "100m"
      limits:
        memory: "1024Mi"
        cpu: "2000m"
    jobs:
      tests:
        limits:
          memory: "1024Mi"
          cpu: "2000m"
        requests:
          memory: "128Mi"
          cpu: "100m"
      prometheus_create_mysql_user:
        limits:
          memory: "1024Mi"
          cpu: "2000m"
        requests:
          memory: "128Mi"
          cpu: "100m"
      image_repo_sync:
        requests:
          memory: "128Mi"
          cpu: "100m"
        limits:
          memory: "1024Mi"
          cpu: "2000m"
      mariadb_phy_backup:
        requests:
          memory: "128Mi"
          cpu: "100m"
        limits:
          memory: "1024Mi"
          cpu: "2000m"
      mariadb_phy_restore:
        requests:
          memory: "128Mi"
          cpu: "100m"
        limits:
          memory: "1024Mi"
          cpu: "2000m"
  tolerations:
    server:
      tolerations: []

dependencies:
  dynamic:
    common:
      local_image_registry:
        jobs:
          - mariadb-image-repo-sync
        services:
          - endpoint: node
            service: local_image_registry
  static:
    error_pages:
      jobs: null
    ingress:
      jobs: null
      services:
        - endpoint: error_pages
          service: oslo_db
    mariadb:
      jobs: null
      services: null
    prometheus_create_mysql_user:
      services:
        - endpoint: internal
          service: oslo_db
    prometheus_mysql_exporter:
      jobs:
        - mariadb-exporter-create-sql-user
      services:
        - endpoint: internal
          service: oslo_db
    prometheus_mysql_exporter_tests:
      services:
        - endpoint: internal
          service: prometheus_mysql_exporter
        - endpoint: internal
          service: monitoring
    image_repo_sync:
      services:
        - endpoint: internal
          service: local_image_registry
    cluster_wait:
      services:
        - endpoint: internal
          service: oslo_db
    tests:
      services:
        - endpoint: internal
          service: oslo_db
    controller:
      services: null
volume:
  # this value is used for single pod deployments of mariadb to prevent losing all data
  # if the pod is restarted
  use_local_path_for_single_pod_cluster:
    enabled: false
    host_path: "/tmp/mysql-data"
  chown_on_start: true
  enabled: true
  class_name: general
  size: 5Gi
  phy_backup:
    enabled: false
    class_name: general
    size: 5Gi
    # pv_nfs options
    # nfs:
    #   server: 127.0.0.1
    #   path: /share
    #   mountOptions:
    #     - "nfsvers=4"
    #     - "hard"
    #     - "sec=sys:krb5:none"

jobs:
  cluster_wait:
    clusterCheckWait: 30
    clusterCheckRetries: 30
    clusterStabilityCount: 30
    clusterStabilityWait: 4
  exporter_create_sql_user:
    backoffLimit: 87600
    activeDeadlineSeconds: 3600
  phy_backup_mariadb:
    cron: "0 0 * * *"
    history:
      success: 3
      failed: 1
    suspend: false
  backup_mariadb:
    cron: "0 0 * * *"
    history:
      success: 3
      failed: 1

network:
  mariadb: {}
  mariadb_discovery: {}
  mariadb_ingress: {}
  mariadb_ingress_error_pages: {}
  mariadb_master: {}

conf:
  tests:
    # This may either be:
    # * internal: which will hit the endpoint exposed by the ingress controller
    # * direct: which will hit the backends directly via a k8s service ip
    # Note, deadlocks and failure are to be expected with concurrency if
    # hitting the `direct` endpoint.
    endpoint: internal
    # This is a list of tuning params passed to mysqlslap:
    params:
      - --auto-generate-sql
      - --concurrency=100
      - --number-of-queries=1000
      - --number-char-cols=1
      - --number-int-cols=1
  ingress: null
  ingress_conf:
    worker-processes: "auto"
  rclone: {}
    # remote1:
    #   type: s3
    #   provider: Ceph
    #   access_key_id: EQWLB53ZR10068X4DG1J
    #   secret_access_key: 0GyHi6hPTDqEqZm2gvlchgx6wO9maBEKSR72dAZk
    #   endpoint: https://openstack-store.it.just.works/
    #   upload_cutoff: 0

  # env variables will be injected as for example:
  # RCLONE_NO_CHECK_CERTIFICATE=True
  # for all rclone commands
  rclone_env: {}
    #  no_check_certificate: true
    #  log_level: DEBUG
  phy_backup:
    enabled: true
    # supported backends: pvc, pv_nfs, hostpath
    backend: pvc
    # used only with backuend: hostpath
    host_path: /var/lib/openstack-helm/mariadb-backup
    backup_pvc_name: mariadb-phy-backup-data
    # each week (604800 seconds) new full backup is created, else
    # create incremental to latest full backup
    backup_type: incremental
    # will be removed backups older than 10*604800 seconds (10 weeks)
    backups_to_keep: 10
    full_backup_cycle: 604800
    # 6 hours default backup timeout
    backup_timeout: 21600
    # 10 minutes in order to put mariadb back to sync and
    # cleanup backup runner pod
    backup_timeout_delta: 600
    # The REQUIRED_SPACE_RATIO is a multiplier for database size for predicting space needed
    # to create backup (full or incremental) and then to do a restore keeping uncompressed backup files
    # on the same filesystem as compressed ones. To estimate how big REQUIRED_SPACE_RATIO can be the next
    # formula can be used:
    # size of (1 uncompressed full backup + all related incremental
    # uncompressed backups + 1 full compressed backup) in KB =< (DB_SIZE * REQUIRED_SPACE_RATIO) in KB
    backup_required_space_ratio: 1.2
    sync_remote:
      enabled: false
      # remote: remote1
      # path: testbucket/path/to/dir
    openssl_encryption: false
    openssl_kek: '45dcb3a162b5929f2778d130cc0abf57e8f7c1e7d847ce22d50d34460c00274d'
  phy_restore:
    enabled: true
    backup_name: ""
    # 2 hours default restore timeout for 1 mariadb replica
    replica_restore_timeout: 3600
    # delta includes time to scale mariadb to 0 replicas before restore
    # and rescale back to MARIADB_REPLICAS after
    restore_timeout_delta: 1800
    sts_scale_timeout: 3000
  database_conf:
    mysqld:
      wsrep_provider_options:
        evs.suspect_timeout: "PT30S"
        gmcast.peer_timeout: "PT15S"
  database:
    my: |
      [mysqld]
      datadir=/var/lib/mysql
      basedir=/usr
      ignore-db-dirs=lost+found

      [client-server]
      !includedir /etc/mysql/conf.d/
    00_base: |
      [mysqld]
      # Charset
      character_set_server=utf8
      collation_server=utf8_general_ci
      skip-character-set-client-handshake

      # Logging
      slow_query_log=off
      slow_query_log_file=/var/log/mysql/mariadb-slow.log
      log_warnings=2

      # General logging has huge performance penalty therefore is disabled by default
      general_log=off
      general_log_file=/var/log/mysql/mariadb-error.log

      long_query_time=3
      log_queries_not_using_indexes=on

      # Networking
      bind_address=0.0.0.0
      port={{ tuple "oslo_db" "direct" "mysql" . | include "helm-toolkit.endpoints.endpoint_port_lookup" }}

      # When a client connects, the server will perform hostname resolution,
      # and when DNS is slow, establishing the connection will become slow as well.
      # It is therefore recommended to start the server with skip-name-resolve to
      # disable all DNS lookups. The only limitation is that the GRANT statements
      # must then use IP addresses only.
      skip_name_resolve

      # Tuning
      user=mysql
      max_allowed_packet=256M
      open_files_limit=10240
      max_connections=8192
      max-connect-errors=1000000

      ## Generally, it is unwise to set the query cache to be larger than 64-128M
      ## as the costs associated with maintaining the cache outweigh the performance
      ## gains.
      ## The query cache is a well known bottleneck that can be seen even when
      ## concurrency is moderate. The best option is to disable it from day 1
      ## by setting query_cache_size=0 (now the default on MySQL 5.6)
      ## and to use other ways to speed up read queries: good indexing, adding
      ## replicas to spread the read load or using an external cache.
      query_cache_size=0
      query_cache_type=0

      sync_binlog=0
      thread_cache_size=16
      table_open_cache=2048
      table_definition_cache=1024

      #
      # InnoDB
      #
      # The buffer pool is where data and indexes are cached: having it as large as possible
      # will ensure you use memory and not disks for most read operations.
      # Typical values are 50..75% of available RAM.
      # TODO(tomasz.paszkowski): This needs to by dynamic based on available RAM.
      innodb_buffer_pool_size=2048M
      innodb_doublewrite=0
      innodb_file_format=Barracuda
      innodb_file_per_table=1
      innodb_flush_method=O_DIRECT
      innodb_io_capacity=500
      innodb_locks_unsafe_for_binlog=1
      innodb_log_file_size=128M
      innodb_old_blocks_time=1000
      innodb_read_io_threads=8
      innodb_write_io_threads=8

      # Clustering
      binlog_format=ROW
      default-storage-engine=InnoDB
      innodb_autoinc_lock_mode=2
      innodb_flush_log_at_trx_commit=2
      wsrep_cluster_name={{ tuple "oslo_db" "direct" . | include "helm-toolkit.endpoints.hostname_namespaced_endpoint_lookup" | replace "." "_" }}
      wsrep_on=1
      wsrep_provider=/usr/lib/galera/libgalera_smm.so
      {{- $wsrep_provider_options := list }}
      {{- range $key, $value := .Values.conf.database_conf.mysqld.wsrep_provider_options }}
         {{- $wsrep_provider_options = append $wsrep_provider_options (printf "%s=%s" $key $value) }}
      {{- end }}
      wsrep_provider_options="gmcast.listen_addr=tcp://0.0.0.0:{{ tuple "oslo_db" "direct" "wsrep" . | include "helm-toolkit.endpoints.endpoint_port_lookup" }}; {{ $wsrep_provider_options | join "; " }}"
      # Use one thread, should make cluster more stable during unstable networking MDEV-28452 and PRODX-41195
      wsrep_slave_threads=1
      wsrep_sst_auth={{ .Values.endpoints.oslo_db.auth.sst.username }}:{{ .Values.endpoints.oslo_db.auth.sst.password }}
      wsrep_sst_method=mariabackup

      [mysqldump]
      max-allowed-packet=16M

      [client]
      default_character_set=utf8
      protocol=tcp
      port={{ tuple "oslo_db" "direct" "mysql" . | include "helm-toolkit.endpoints.endpoint_port_lookup" }}

      # https://jira.mariadb.org/browse/MDEV-18621
      [sst]
      sockopt=,keepalive,keepidle=10,keepintvl=10,keepcnt=3
    config_override: null
    # Any configuration here will override the base config.
    # config_override: |-
    #   [mysqld]
    #   wsrep_slave_threads=1
    99_force: |
      [mysqld]
      datadir=/var/lib/mysql
      tmpdir=/tmp

monitoring:
  prometheus:
    enabled: false
    mysqld_exporter:
      scrape: true

# typically overridden by environmental
# values, but should include all endpoints
# required by this chart
endpoints:
  cluster_domain_suffix: cluster.local
  local_image_registry:
    name: docker-registry
    namespace: docker-registry
    hosts:
      default: localhost
      internal: docker-registry
      node: localhost
    host_fqdn_override:
      default: null
    port:
      registry:
        node: 5000
  monitoring:
    name: prometheus
    namespace: null
    hosts:
      default: prom-metrics
      public: prometheus
    host_fqdn_override:
      default: null
    path:
      default: null
    scheme:
      default: 'http'
    port:
      api:
        default: 9090
        public: 80
  prometheus_mysql_exporter:
    namespace: null
    hosts:
      default: mysql-exporter
    host_fqdn_override:
      default: null
    path:
      default: /metrics
    scheme:
      default: 'http'
    port:
      metrics:
        default: 9104
  oslo_db:
    namespace: null
    auth:
      admin:
        username: root
        password: password
      mariabackup:
        username: mariabackup
        password: password
      sst:
        username: sst
        password: password
      audit:
        username: audit
        password: password
      exporter:
        username: exporter
        password: password
    hosts:
      default: mariadb
      direct: mariadb-server
      discovery: mariadb-discovery
      error_pages: mariadb-ingress-error-pages
    host_fqdn_override:
      default: null
    path: null
    scheme: mysql+pymysql
    port:
      mysql:
        default: 3306
      wsrep:
        default: 4567
      ist:
        default: 4568
      sst:
        default: 4444
  kube_dns:
    namespace: kube-system
    name: kubernetes-dns
    hosts:
      default: kube-dns
    host_fqdn_override:
      default: null
    path:
      default: null
    scheme: http
    port:
      dns_tcp:
        default: 53
      dns:
        default: 53
        protocol: UDP
  mariadb_controller:
    name: mariadb-controller
    port:
      health:
        default: 8080

network_policy: {}

manifests:
  configmap_bin: true
  configmap_etc: true
  configmap_ingress_conf: true
  configmap_ingress_etc: true
  configmap_services_tcp: true
  deployment_error: true
  deployment_ingress: true
  deployment_controller: false
  job_image_repo_sync: true
  job_cluster_wait: false
  # flag cron_job_mariadb_backup enables cron job
  # responsible for logical backup of mariadb using
  # mysqldump tool.
  # flag cron_job_mariadb_phy_backup enables cron job
  # responsible for physical backup of mariadb replica
  # using mariabackup tool. Both flags could be enabled
  # at the same carefully because schedule times can overlap.
  cron_job_mariadb_phy_backup: false
  job_mariadb_phy_restore: false
  monitoring:
    prometheus:
      configmap_bin: true
      job_user_create: true
      secret_etc: true
      service_exporter: true
      network_policy_exporter: false
  pdb_server: true
  network_policy: false
  pod_test: true
  secret_ca_bundle: false
  secret_mariabackup_password: true
  secret_dbadmin_password: true
  secret_sst_password: true
  secret_dbaudit_password: true
  secret_etc: true
  service_discovery: true
  service_ingress: true
  service_error: true
  service_master: false
  service: true
  statefulset: true
...
