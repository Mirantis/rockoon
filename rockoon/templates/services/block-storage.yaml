#apiVersion: lcm.mirantis.com/v1alpha1
#kind: HelmBundle

{%- from 'macros/messaging_service_creds.j2' import messaging_service_creds %}
{%- set service = 'cinder' %}
{%- set stacklight_enabled = spec.get('features', {}).get('stacklight', {}).get('enabled', False) %}
{%- set notification_topics = ['notifications'] %}
{%- do notification_topics.append('stacklight_notifications') if stacklight_enabled %}
{%- set external_notifications_enabled = spec.get('features', {}).get('messaging', {}).get('notifications', {}).get('external', {}).get('enabled', False) %}
{%- if external_notifications_enabled %}
  {%- for topic in spec.get('features', {}).get('messaging', {}).get('notifications', {}).get('external', {}).get('topics', []) %}
    {%- do notification_topics.append(topic) %}
  {%- endfor %}
{%- endif  %}
{%- set node_specific = {} %}
{%- set overrides = namespace(enabled=false) %}
{%- for label, node_features in spec.get("nodes", {}).items() %}
  {%- if "cinder" in node_features.get("features", {}).keys() %}
    {% set overrides.enabled = true %}
  {%- endif  %}
  {%- if node_features.get("features", {}) %}
    {%- do node_specific.update({label: node_features.features}) %}
  {%- endif %}
{%- endfor %}
{%- set glance_cinder_backends = spec.get('features', {}).get('glance', {}).get('backends', {}).get('cinder', {}) %}
{%- set glance_cinder_multi = {} %}
{%- set cinder_db_cleanup = spec.get('features', {}).get('database', {}).get('cleanup', {}).get('cinder', {'enabled': true}) %}
{%- from 'macros/etcd3.j2' import get_etcd3_endpoint %}
{%- set cadf_audit = spec.get('features', {}).get('logging', {}).get('cadf', {'enabled': false}) %}
{%- set cadf_audit_driver = spec.get('features', {}).get('logging', {}).get('cadf', {}).get('driver', 'messagingv2') %}
{%- set is_backup_enabled = spec.get('features', {}).get('cinder', {}).get('backup', {}).get('enabled', True) %}
{%- if is_backup_enabled %}
{%- set backup_drivers = [] %}
{%-   for driver_name, driver in spec.get('features', {}).get('cinder', {}).get('backup', {}).get('drivers', {}).items() if driver.get('enabled', False) %}
{%-     do backup_drivers.append(driver) %}
{%-   endfor %}
# NOTE: we allow only one enabled backup driver at the moment in admission. More than 1 driver will require own statefulsets which is not supported at the moment.
{%-   if backup_drivers %}
{%-     set main_backup_driver = backup_drivers[0] %}
{%-   endif %}
{%- endif %}
{%- set extra_volumes = spec.get('features', {}).get('cinder', {}).get('volume', {}).get('backends', {}) %}
{%- set extra_volumes_sts = {} %}
{%- set extra_bootstrap = {} %}
{%- for sts_name, sts_opts in extra_volumes.items() %}
{%-   if sts_opts.get('enabled', True) %}
{%-     do extra_volumes_sts.update({sts_name:{'values': sts_opts['values']}}) %}
{%-     if sts_opts.get("create_volume_type", True) %}
{%-       set enabled_backends = sts_opts["values"]["conf"]["cinder"]["DEFAULT"]["enabled_backends"].split(',') %}
{%-       for section, opts in sts_opts["values"]["conf"]["cinder"].items() %}
{%-         if section in enabled_backends %}
{%-           set volume_backend_name = opts["volume_backend_name"] %}
{%-           do extra_bootstrap.update({volume_backend_name: {"volume_backend_name": volume_backend_name}}) %}
{%-         endif %}
{%-       endfor %}
{%-     endif %}
{%-   endif %}
{%- endfor %}


spec:
  releases:
{%- if spec.get('migration', {}).get('cinder', {}).get('deploy_main_service', True) %}
  - name: openstack-cinder
    chart: cinder
    values:
      images:
        tags:
{%- for image in [
    "db_drop",
    "image_repo_sync",
    "cinder_api",
    "cinder_scheduler",
    "db_init",
    "dep_check",
    "cinder_db_sync",
    "cinder_db_sync_online",
    "cinder_db_purge",
    "cinder_backup",
    "ks_user",
    "ks_service",
    "cinder_volume_usage_audit",
    "cinder_backup_storage_init",
    "ks_endpoints",
    "bootstrap",
    "cinder_storage_init",
    "rabbit_init",
    "cinder_service_cleaner",
    "cinder_volume",
    "cinder_volume_daemonset",
    "cinder_drop_default_volume_type",
    "cinder_create_internal_tenant",
    "cinder_wait_for_backends",
    "test",] %}
        {%- if image in images %}
          {{ image }}: {{ images[image] }}
        {%- endif %}
{%- endfor %}
      dependencies:
        static:
          db_init:
            jobs:
              - openstack-mariadb-cluster-wait
      pod:
        replicas:
          api: 1
          registry: 1
        affinity:
          anti:
            type:
              backup: "requiredDuringSchedulingIgnoredDuringExecution"
{%- if overrides.enabled %}
        # migration process from source RBD based volume (hosted by sts) to
        # destination ISCSI/LVM volume requires host networking to properly
        # make an iscsi discover and login
        useHostNetwork:
          volume: true
{%- endif %}
        security_context:
          # NOTE(vsaienko): run volume and backup with same user
          # to allow collocate ceph based volume with backups and avoid issues
          # with /var/lib/cinder shared state directory
{%- set cinder_user = 42424 %}
          cinder_volume:
            pod:
              runAsUser: {{ cinder_user }}
          # NOTE(okononenko): set containers to privileged mode to work with
          # iSCSI/LVM and multipath services
{%- if overrides.enabled %}
            container:
              cinder_volume:
                privileged: true
{%- endif %}
          cinder_backup:
            pod:
              runAsUser: {{ cinder_user }}
{%- if overrides.enabled %}
            container:
              cinder_backup:
                privileged: true
          cinder_volume_daemonset:
            pod:
              runAsUser: {{ cinder_user }}
            container:
              cinder_volume:
                privileged: true
{%- endif %}
{%- if is_ceph_enabled %}
      storage: ceph
      ceph_client:
        configmap: rook-ceph-config
        user_secret_name: {{ ceph.cinder.secrets }}
{%- endif %}
      conf:
        policy.d:
          01-controller-default.yaml: {{ service_policy }}
          02-custom.yaml: {{ spec.get("features", {}).get("policies", {}).get("cinder", {}) }}
        {%- if overrides.enabled %}
        standalone_backends:
          daemonset:
            conf:
              DEFAULT:
                cluster: ""
        {%- endif %}
        backends:
{%- set enabled_backends=[] %}
{%- if is_ceph_enabled %}
  {%- for backend, backend_config in ceph.cinder.pools.items() %}
    {%- if backend_config.role == 'volumes' %}
      {%- do enabled_backends.append(backend) %}
          {{ backend }}:
      {%- for g_backend_name, g_backend_opts in glance_cinder_backends.items() %}
        {%- if g_backend_opts.get('backend_name') %}
          {%- set glance_cinder_volume_type, glance_cinder_backend_name =  g_backend_opts['backend_name'].split(':') %}
        {%- endif %}
        {%- if glance_cinder_volume_type == 'rbd' and glance_cinder_backend_name == backend %}
            image_upload_use_cinder_backend: True
            image_upload_use_internal_tenant: True
        {%- endif %}
      {%- endfor %}
            volume_driver: cinder.volume.drivers.rbd.RBDDriver
            volume_backend_name: {{ backend }}
            rbd_pool: {{ backend_config.name }}
            rbd_user: {{ ceph.cinder.username }}
            rbd_ceph_conf: "/etc/ceph/ceph.conf"
    {%- endif %}
  {%- endfor %}
        ceph:
          keyrings:
            {{ ceph.cinder.username }}:
              key: {{ ceph.cinder.keyring }}
          pools:
            backup:
              replication: 1
              crush_rule: replicated_ruleset
              chunk_size: 8
            cinder.volumes:
              replication: 1
              crush_rule: replicated_ruleset
              chunk_size: 8
          config:
            global:
              mon_host: {{ ceph.mon_host }}
{%- endif %}
        cinder:
          keystone_authtoken:
            memcache_security_strategy: ENCRYPT
            memcache_secret_key: {{ credentials[0].memcached }}
            {%- if OSVer[spec.openstack_version] >= OSVer.train %}
            # TODO(pas-ha) register block-storage service type and catalog entry as well
            service_type: volumev3
            {%- endif %}
          DEFAULT:
            # use unique host for both cluster and non-cluster mode,
            # otherwise messaging between API and cinder-volume will
            # be broken
            host: "<None>"
# NOTE(vsaienko): active/active mode is supported by rbd starting from Rocky
# but in rocky it is not stable, so apply only from stein
{%- if is_ceph_enabled %}
  {%- if OSVer[spec.openstack_version] >= OSVer.stein %}
            cluster: "cinder-ceph-cluster"
  {%- endif %}
{%- endif %}
{%- if is_backup_enabled %}
  {%- if not backup_drivers %}
            backup_driver: cinder.backup.drivers.ceph.CephBackupDriver
    {%- for backend, backend_config in ceph.cinder.pools.items() %}
      {%- if backend_config.role == 'backup' %}
            backup_ceph_user: {{ ceph.cinder.username }}
            backup_ceph_pool: {{ backend_config.name }}
      {%- endif %}
    {%- endfor %}
  {%- elif main_backup_driver["type"] == "s3" %}
            backup_driver: cinder.backup.drivers.s3.S3BackupDriver
    {%- for param, param_val in main_backup_driver.items() %}
      {%- if param in ["endpoint_url", "store_bucket", "store_access_key", "store_secret_key"] %}
            backup_s3_{{ param }}: {{ param_val }}
      {%- endif %}
    {%- endfor %}
  {%- elif main_backup_driver["type"] == "nfs" %}
            backup_driver: cinder.backup.drivers.nfs.NFSBackupDriver
            backup_share: {{ main_backup_driver["backup_share"] }}
  {%- endif %}
{%- elif OSVer[spec.openstack_version] >= OSVer.yoga %}
            backup_api_class: cinder.backup.api.NoOp
{%- endif %}
            control_exchange: cinder
{%- if is_ceph_enabled %}
            enabled_backends: {{ enabled_backends|join(',') }}
            default_volume_type: {{ enabled_backends[0] }}
{%- endif %}
            scheduler_default_filters: AvailabilityZoneFilter,CapacityFilter,CapabilitiesFilter,InstanceLocalityFilter
{%- if cadf_audit.enabled %}
          audit_middleware_notifications:
            driver: {{ cadf_audit_driver }}
{%- else %}
          audit_middleware_notifications:
            driver: noop
{%- endif %}
          oslo_messaging_notifications:
            topics: {{ notification_topics|join(',') }}
          oslo_middleware:
            max_request_body_size: 114688
{% include 'base/_oslo_policy_enforce_defaults.yaml' %}
          coordination:
            #There are some problems with etcd3 driver so we shouldn't use it
            #BUG: https://mirantis.jira.com/browse/PRODX-21783
            backend_url: {{ get_etcd3_endpoint(spec.openstack_version, 'etcd3gw') }}
          service_user:
            send_service_user_token: true
          backend_defaults:
            # NOTE(vsaienko): Use dedicated pool for cinder only, this will allow to improve
            # scale characteristics
            rbd_exclusive_cinder_pool: true
            # NOTE(vsaienko): always specify rbd_secret_uuid as since Antelope this is set
            # by default on cinder cide. Should be matched with values in nova/cinder keyrings
            rbd_secret_uuid: c9543a17-411c-45e1-8a25-3f03e0090dc2
            report_dynamic_total_capacity: false
          # Needed for InstanceLocalityFilter
          nova:
            interface: internal
            auth_section: keystone_authtoken
            auth_type: password
{%- if spec.get('features', {}).get('glance', {}).get("signature", {}).get("enabled", False) %}
          glance:
            verify_glance_signatures: true
{%- endif %}
        # NOTE(okononenko): We need to enable iSCSI support when use iSCSI/LVM backend
        enable_iscsi: {{ overrides.enabled }}
        logging:
          logger_cinder:
            level: {{ spec.get('features', {}).get('logging', {}).get('cinder', {}).get('level', 'INFO') }}
          logger_os.brick:
            level: {{ spec.get('features', {}).get('logging', {}).get('cinder', {}).get('level', 'INFO') }}
      # NOTE(vsaienko): do not create backends from .Values.conf.cinder.DEFAULT.backends
      # as we have default backend in chart values rbd1 which is not used.
      # Do not create volume type for backup, as user can't use this backend directly.
      bootstrap:
        bootstrap_conf_backends: false
        volume_types:
    {%- for g_backend_name, g_backend_opts in glance_cinder_backends.items() %}
      {%- if g_backend_opts.get('backend_name') %}
        {%- set glance_cinder_volume_type, glance_cinder_backend_name =  g_backend_opts['backend_name'].split(':') %}
          {{ glance_cinder_backend_name }}_multiattach:
            volume_backend_name: {{ glance_cinder_backend_name }}
            multiattach: "\"<is> True\""
      {%- endif %}
    {%- endfor %}
{%- for backend in enabled_backends %}
          {{ backend }}:
            volume_backend_name: {{ backend }}
          {{ backend }}_multiattach:
            volume_backend_name: {{ backend }}
            multiattach: "\"<is> True\""
{%- endfor %}
{%- for label, override in node_specific.items() %}
  {%- set cinder_override = override.get("cinder", {}) %}
    {%- if cinder_override %}
      {%- for backend_name,backend_opts in cinder_override.get("volume", {}).get("backends", {}).items() %}
          {{ backend_name }}:
            volume_backend_name: {{ backend_name }}
        {%- if "lvm" in backend_opts %}
          {{ backend_name }}_multiattach:
            volume_backend_name: {{ backend_name }}
            multiattach: "\"<is> True\""
        {%- endif %}
      {%- endfor %}
    {%- endif %}
{%- endfor %}
{%- if extra_bootstrap %}
          {{ extra_bootstrap | toyaml | indent(10) }}
{%- endif %}
{%- if is_ceph_enabled %}
      secrets:
        rbd:
          volume: {{ ceph.cinder.secrets }}
  {%- if is_backup_enabled and not backup_drivers %}
          backup: {{ ceph.cinder.secrets }}
  {%- endif %}
{%- endif %}
{%- if spec.features.network_policies.enabled %}
      network_policy:
{% include 'base/network_policies/block-storage.yaml' %}
{%- endif %}
      manifests:
        cron_job_db_purge: true
        {%- if OSVer[spec.openstack_version] >= OSVer.victoria %}
        job_drop_default_volume_type: true
        {%- endif %}
        network_policy: {{ spec.features.network_policies.enabled }}
        job_rabbit_init: false
        job_storage_init: false
        job_backup_storage_init: false
        secret_ca_bundle: true
        cron_service_cleaner: true
        cron_volume_usage_audit: false
        statefulset_backup: {{ is_backup_enabled }}
        statefulset_volume: {{ spec.features.get("cinder", {}).get("volume", {}).get("enabled", True) }}
        ceph_conf: {{ is_ceph_enabled }}
        job_clean: false
      network:
        api:
          ingress:
            annotations:
              nginx.ingress.kubernetes.io/proxy-body-size: "114688"
      # NODE SPECIFIC START
      {%- if overrides.enabled or extra_volumes_sts %}
      overrides:
        {%- if overrides.enabled %}
        cinder_volume_ds:
          labels:
          {%- for label, override in node_specific.items() %}
            {%- set cinder_override = override.get("cinder", {}) %}
            {%- if cinder_override %}
            {{ label }}:
            {%- endif %}
             values:
               conf:
                 standalone_backends:
                   daemonset:
                     conf:
                       {%- set overriden_enabled_backends = [] %}
                       {%- for backend_name,backend_opts in cinder_override.get("volume", {}).get("backends", {}).items() %}
                       {%- do overriden_enabled_backends.append(backend_name) %}
                       {{ backend_name }}:
                         {%- if "lvm" in backend_opts %}
                         volumes_dir: /var/lib/cinder/volumes
                         volume_driver: cinder.volume.drivers.lvm.LVMVolumeDriver
                         volume_backend_name: {{ backend_name }}
                     {%- for g_backend_name, g_backend_opts in glance_cinder_backends.items() %}
                       {%- if g_backend_opts.get('backend_name') %}
                         {%- set glance_cinder_volume_type, glance_cinder_backend_name =  g_backend_opts['backend_name'].split(':') %}
                       {%- endif %}
                       {%- if glance_cinder_volume_type == 'lvm' and glance_cinder_backend_name == backend_name %}
                         image_upload_use_cinder_backend: True
                         image_upload_use_internal_tenant: True
                       {%- endif %}
                     {%- endfor %}
                         {%- for key,val in backend_opts.lvm.items() %}
                         {{ key }}: {{ val }}
                         {%- endfor %}
                           {%- if "target_helper" not in backend_opts.lvm %}
                         target_helper: lioadm
                           {%- endif %}
                         {%- endif %}
                       DEFAULT:
                         enabled_backends: {{ ','.join(overriden_enabled_backends) }}
                       {%- for g_backend_name, g_backend_opts in glance_cinder_backends.items() %}
                       {%- if g_backend_opts.get('backend_name') %}
                         {%- set glance_cinder_volume_type, glance_cinder_backend_name =  g_backend_opts['backend_name'].split(':') %}
                       {%- endif %}
                         {%- if glance_cinder_backend_name in overriden_enabled_backends %}
                         allowed_direct_url_schemes: cinder
                         {%- endif %}
                       {%- endfor %}
                       {%- endfor %}
          {%- endfor %}
        {%- endif %}
      {%- endif %}
      {%- if extra_volumes_sts %}
        cinder_volume_sts:
          {{ extra_volumes_sts | toyaml | indent(10) }}
      {%- endif %}
      # NODE SPECIFIC END
      endpoints:
        cluster_domain_suffix: {{ spec.internal_domain_name }}
{% include 'base/_admin_identity.yaml' %}
{% include 'base/_cache.yaml' %}
        oslo_db:
          auth:
            admin:
              username: {{ admin_creds.database.username }}
              password: {{ admin_creds.database.password }}
            cinder:
              username: {{ credentials[0].database.user.username }}
              password: {{ credentials[0].database.user.password }}
{% include 'base/_messaging_shared.yaml' %}
{% include 'base/_notifications.yaml' %}
        volume:
          enabled: false
        volumev2:
{%- if OSVer[spec.openstack_version] >= OSVer.xena %}
          state:  absent
{%- endif %}
          host_fqdn_override:
            public:
              host: cinder.{{ spec.public_domain_name }}
              tls:
                ca: |
{{ spec.features.ssl.public_endpoints.ca_cert | indent( width=18, first=True) }}
                crt: |
{{ spec.features.ssl.public_endpoints.api_cert | indent( width=18, first=True) }}
                key: |
{{ spec.features.ssl.public_endpoints.api_key | indent( width=18, first=True) }}
          hosts:
            admin:
              host: cinder-api
            default: cinder
            internal: cinder-api
            public:
              host: cinder
              tls:
                ca: |
{{ spec.features.ssl.public_endpoints.ca_cert | indent( width=18, first=True) }}
                crt: |
{{ spec.features.ssl.public_endpoints.api_cert | indent( width=18, first=True) }}
                key: |
{{ spec.features.ssl.public_endpoints.api_key | indent( width=18, first=True) }}
          port:
            api:
              admin: 8776
              default: 80
              internal: 8776
              public: 443
          scheme:
            default: http
            public: https
        volumev3:
          host_fqdn_override:
            public:
              host: cinder.{{ spec.public_domain_name }}
              tls:
                ca: |
{{ spec.features.ssl.public_endpoints.ca_cert | indent( width=18, first=True) }}
{%- if proxy_settings is defined %}
{%- if proxy_settings.get("proxy_ca_certificate") %}
{{ proxy_settings["proxy_ca_certificate"] | indent( width=18, first=True) }}
{%- endif %}
{%- endif %}
                crt: |
{{ spec.features.ssl.public_endpoints.api_cert | indent( width=18, first=True) }}
                key: |
{{ spec.features.ssl.public_endpoints.api_key | indent( width=18, first=True) }}
          hosts:
            admin:
              host: cinder-api
            default: cinder
            internal: cinder-api
            public:
              host: cinder
              tls:
                ca: |
{{ spec.features.ssl.public_endpoints.ca_cert | indent( width=18, first=True) }}
                crt: |
{{ spec.features.ssl.public_endpoints.api_cert | indent( width=18, first=True) }}
                key: |
{{ spec.features.ssl.public_endpoints.api_key | indent( width=18, first=True) }}
          port:
            api:
              admin: 8776
              default: 80
              internal: 8776
              public: 443
          scheme:
            default: http
            public: https
      jobs:
{% include 'base/_ks_jobs.yaml' %}
        db_purge:
          enabled: {{ cinder_db_cleanup.enabled }}
          cron: {{ cinder_db_cleanup.get("schedule", "1 0 * * 1") }}
          script:
            config:
              age: {{ cinder_db_cleanup.get("age", 30) }}
{%- endif %}
