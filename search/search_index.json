{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to Rockoon Controller documentation","text":""},{"location":"#introduction","title":"Introduction","text":"<p>The Rockoon Controller is a Kubernetes operator that implements lifecycle management for OpenStack deployment.</p> <p>The Rockoon is written in Python using Kopf as a Python framework to build Kubernetes operators, and Pykube.</p> <p>The controller subscribes to changes to OpenStackDeployment Kubernetes custom resource and then reacts to these changes by creating, updating, or deleting appropriate resources in Kubernetes.</p>"},{"location":"#getting-help","title":"Getting Help","text":"<ul> <li>File a bug: https://github.com/Mirantis/rockoon/issues</li> </ul>"},{"location":"#developer","title":"Developer","text":"<ul> <li>Contributing: https://TODO</li> <li>Reference Architecture:  https://mirantis.github.io/rockoon</li> </ul>"},{"location":"developer/","title":"Developer Guide","text":""},{"location":"developer/#code-style","title":"Code Style","text":"<p>Rockoon Contoller uses Black code formatter To check your chenages and format them use <pre><code>tox -e black\n</code></pre></p>"},{"location":"developer/#tests","title":"Tests","text":"<p>Each commit should require to pass code styles and unittests. To run unittests locally <pre><code>tox -e py310\n</code></pre></p>"},{"location":"developer/#running-controller-locally","title":"Running controller locally","text":"<p>Rockoon Controller is deployed as helm chart into kubernetes cluster. However there is possibility to run controller locally. For this: <pre><code>tox -e dev\n</code></pre></p>"},{"location":"architecture/rockoon-admission/","title":"OpenStack Controller Admission","text":"<p>The CustomResourceDefinition resource in Kubernetes uses the OpenAPI Specification version 2 to specify the schema of the resource defined. The Kubernetes API outright rejects the resources that do not pass this schema validation.</p> <p>The language of the schema, however, is not expressive enough to define a specific validation logic that may be needed for a given resource. For this purpose, Kubernetes enables the extension of its API with Dynamic Admission Control.</p> <p>For the OpenStackDeployment (OsDpl) CR the ValidatingAdmissionWebhook is a natural choice. It is deployed as part of OpenStack Controller in dedicated deployment by default and performs specific extended validations when an <code>OpenStackDeployment</code> CR is created or updated.</p> <p>The inexhaustive list of additional validations includes:</p> <ul> <li>Deny the OpenStack version downgrade</li> <li>Deny the OpenStack version skip-level upgrade</li> <li>Deny the OpenStack master version deployment</li> <li>Deny upgrade to the OpenStack master version</li> <li>Deny deploying invalid configuration</li> </ul>"},{"location":"architecture/custom-resources/openstackdeployment/","title":"OpenStackDeployment Custom Resource","text":"<p>Custom Kubernetes resource that describes OpenStack deployment.</p> <pre><code>kubectl get crd openstackdeployments.lcm.mirantis.com -o yaml\n</code></pre> <pre><code>kubectl -n openstack get osdpl -o yaml\n</code></pre> <p>Example of OpenStackDeployment minimal configuration</p> <pre><code>apiVersion: lcm.mirantis.com/v1alpha1\nkind: OpenStackDeployment\nmetadata:\n  annotations:\n  name: osh-dev\n  namespace: openstack\nspec:\n  features:\n    glance:\n      backends:\n        file:\n          pvcstore:\n            default: true\n            pvc:\n              size: 10Gi\n              storage_class_name: lvp-fake-root\n    network_policies:\n      enabled: false\n    neutron:\n      external_networks:\n      - bridge: br-ex\n        interface: veth-phy\n        network_types:\n        - flat\n        physnet: physnet1\n      floating_network:\n        enabled: true\n        physnet: physnet1\n        subnet:\n          gateway: 10.11.12.11\n          pool_end: 10.11.12.200\n          pool_start: 10.11.12.100\n          range: 10.11.12.0/24\n      tunnel_interface: ens3\n    nova:\n      images:\n        backend: local\n      live_migration_interface: ens3\n    services: []\n    ssl:\n      public_endpoints:\n        api_cert:\n          value_from:\n            secret_key_ref:\n              key: api_cert\n              name: osh-dev-hidden\n        api_key:\n          value_from:\n            secret_key_ref:\n              key: api_key\n              name: osh-dev-hidden\n        ca_cert:\n          value_from:\n            secret_key_ref:\n              key: ca_cert\n              name: osh-dev-hidden\n  local_volume_storage_class: lvp-fake-root\n  openstack_version: caracal\n  persistent_volume_storage_class: lvp-fake-root\n  preset: core\n  public_domain_name: it.just.works\n  size: single\n</code></pre>"},{"location":"architecture/custom-resources/openstackdeployment/#main-osdpl-elements","title":"Main osdpl elements","text":"<p>Main elements of OpenStackDeployment custom resource</p> <ul> <li><code>spec.openstack_version</code>: Specifies the OpenStack release to deploy</li> <li><code>spec.preset</code>: String that specifies the name of the preset, a predefined configuration for the OpenStack cluster. A preset includes:<ul> <li>A set of enabled services that includes virtualization, bare metal management, secret management, and others</li> <li>Major features provided by the services, such as VXLAN encapsulation of the tenant traffic</li> </ul> </li> <li><code>spec.size</code>: String that specifies the size category for the OpenStack cluster. The size category defines the internal configuration   of the cluster such as the number of replicas for service workers and timeouts, etc.   The list of supported sizes include:<ul> <li><code>single</code>: single node installation</li> <li><code>tiny</code>: for approximately 10 OpenStack Compute nodes</li> <li><code>small</code>:  for approximately 50 OpenStack Compute nodes</li> <li><code>medium</code>: for approximately 300+ OpenStack Compute nodes  </li> </ul> </li> <li><code>spec.public_domain_name</code>: Specifies the public DNS name for OpenStack services. This is a base DNS name that must be accessible and   resolvable by API clients of your OpenStack cloud. It will be present in the OpenStack endpoints as presented by the OpenStack Identity   service catalog. The TLS certificates used by the OpenStack services (see below) must also be issued to this DNS name. </li> <li><code>spec.features</code>: Contains the top-level collections of settings for the OpenStack deployment that potentially target several OpenStack services.   The section where the customizations should take place.</li> </ul>"},{"location":"architecture/custom-resources/openstackdeployment/#handling-sensitive-information","title":"Handling sensitive information","text":"<p>The <code>OpenStackDeployment</code> custom resource enables you to securely store sensitive fields in Kubernetes secrets. To do that, verify that the reference secret is present in the same namespace as the <code>OpenStackDeployment</code> object and the <code>openstack.lcm.mirantis.com/osdpl_secret</code> label is set to <code>true</code>. The list of fields that can be hidden from <code>OpenStackDeployment</code> is limited and defined by the <code>OpenStackDeployment</code> schema.</p> <p>For example, to hide spec:features:ssl:public_endpoints:api_cert, use the following structure:</p> <pre><code>spec:\n  features:\n    ssl:\n      public_endpoints:\n        api_cert:\n          value_from:\n            secret_key_ref:\n              key: api_cert\n              name: osh-dev-hidden\n</code></pre>"},{"location":"architecture/custom-resources/openstackdeploymentstatus/","title":"OpenStackDeploymentStatus Custom Resource","text":"<p>The resource of kind <code>OpenStackDeploymentStatus</code> is a custom resource that describes the status of an OpenStack deployment. To obtain detailed information about the schema of an OpenStackDeploymentStatus custom resource:</p> <pre><code>kubectl get crd openstackdeploymentstatus.lcm.mirantis.com -o yaml\n</code></pre> <p>To obtain the status definition for a particular OpenStack deployment: <pre><code>kubectl -n openstack get osdplst\n</code></pre></p> <p>Example of system response:</p> <pre><code>NAME      OPENSTACK VERSION   CONTROLLER VERSION   STATE     LCM PROGRESS   HEALTH   MOSK RELEASE\nosh-dev   antelope            0.16.1.dev104        APPLIED   20/20          21/22    MOSK 24.1.3\n</code></pre> <p>Where:</p> <ul> <li><code>OPENSTACK VERSION</code> displays the actual OpenStack version of the deployment</li> <li><code>CONTROLLER VERSION</code> indicates the version of the Rockoon controller responsible for the deployment</li> <li><code>STATE</code> reflects the current status of life-cycle management. The list of possible values includes:</li> <li><code>APPLYING</code> indicates that some Kubernetes objects for applications are in the process of being applied</li> <li><code>APPLIED</code> indicates that all Kubernetes objects for applications have been applied to the latest state</li> <li><code>LCM PROGRESS</code> reflects the current progress of STATE in the format X/Y, where X denotes the number of applications with Kubernetes objects applied and in the actual state, and Y represents the total number of applications managed by the Rockoon controller</li> <li><code>HEALTH</code> provides an overview of the current health status of the OpenStack deployment in the format X/Y, where X represents the number of applications with notReady pods, and Y is the total number of applications managed by the Rockoon controller</li> <li><code>MOSK RELEASE</code> displays the current product release of the OpenStack deployment</li> </ul> <p>Example of <code>OpenStackDeploymentStatus</code></p> <pre><code>kind: OpenStackDeploymentStatus\nmetadata:\n  name: osh-dev\n  namespace: openstack\nspec: {}\nstatus:\n  handle:\n    lastStatus: update\n  health:\n    barbican:\n      api:\n        generation: 2\n        status: Ready\n    cinder:\n      api:\n        generation: 2\n        status: Ready\n      backup:\n        generation: 1\n        status: Ready\n      scheduler:\n        generation: 1\n        status: Ready\n      volume:\n        generation: 1\n        status: Ready\n  osdpl:\n    cause: update\n    changes: '((''add'', (''status'',), None, {''watched'': {''ceph'': {''secret'':\n      {''hash'': ''0fc01c5e2593bc6569562b451b28e300517ec670809f72016ff29b8cbaf3e729''}}}}),)'\n    controller_version: 0.5.3.dev12\n    fingerprint: a112a4a7d00c0b5b79e69a2c78c3b50b0caca76a15fe7d79a6ad1305b19ee5ec\n    openstack_version: ussuri\n    state: APPLIED\n    timestamp: \"2021-09-08 17:01:45.633143\"\n  services:\n    baremetal:\n      controller_version: 0.5.3.dev12\n      fingerprint: a112a4a7d00c0b5b79e69a2c78c3b50b0caca76a15fe7d79a6ad1305b19ee5ec\n      openstack_version: ussuri\n      state: APPLIED\n      timestamp: \"2021-09-08 17:00:54.081353\"\n    block-storage:\n      controller_version: 0.5.3.dev12\n      fingerprint: a112a4a7d00c0b5b79e69a2c78c3b50b0caca76a15fe7d79a6ad1305b19ee5ec\n      openstack_version: ussuri\n      state: APPLIED\n      timestamp: \"2021-09-08 17:00:57.306669\"\n    compute:\n      controller_version: 0.5.3.dev12\n      fingerprint: a112a4a7d00c0b5b79e69a2c78c3b50b0caca76a15fe7d79a6ad1305b19ee5ec\n      openstack_version: ussuri\n      state: APPLIED\n      timestamp: \"2021-09-08 17:01:18.853068\"\n    coordination:\n      controller_version: 0.5.3.dev12\n      fingerprint: a112a4a7d00c0b5b79e69a2c78c3b50b0caca76a15fe7d79a6ad1305b19ee5ec\n      openstack_version: ussuri\n      state: APPLIED\n      timestamp: \"2021-09-08 17:01:00.593719\"\n    dashboard:\n      controller_version: 0.5.3.dev12\n      fingerprint: a112a4a7d00c0b5b79e69a2c78c3b50b0caca76a15fe7d79a6ad1305b19ee5ec\n      openstack_version: ussuri\n      state: APPLIED\n      timestamp: \"2021-09-08 17:00:57.652145\"\n    database:\n      controller_version: 0.5.3.dev12\n      fingerprint: a112a4a7d00c0b5b79e69a2c78c3b50b0caca76a15fe7d79a6ad1305b19ee5ec\n      openstack_version: ussuri\n      state: APPLIED\n      timestamp: \"2021-09-08 17:01:00.233777\"\n    dns:\n      controller_version: 0.5.3.dev12\n      fingerprint: a112a4a7d00c0b5b79e69a2c78c3b50b0caca76a15fe7d79a6ad1305b19ee5ec\n      openstack_version: ussuri\n      state: APPLIED\n      timestamp: \"2021-09-08 17:00:56.540886\"\n    identity:\n      controller_version: 0.5.3.dev12\n      fingerprint: a112a4a7d00c0b5b79e69a2c78c3b50b0caca76a15fe7d79a6ad1305b19ee5ec\n      openstack_version: ussuri\n      state: APPLIED\n      timestamp: \"2021-09-08 17:01:00.961175\"\n    image:\n      controller_version: 0.5.3.dev12\n      fingerprint: a112a4a7d00c0b5b79e69a2c78c3b50b0caca76a15fe7d79a6ad1305b19ee5ec\n      openstack_version: ussuri\n      state: APPLIED\n      timestamp: \"2021-09-08 17:00:58.976976\"\n    ingress:\n      controller_version: 0.5.3.dev12\n      fingerprint: a112a4a7d00c0b5b79e69a2c78c3b50b0caca76a15fe7d79a6ad1305b19ee5ec\n      openstack_version: ussuri\n      state: APPLIED\n      timestamp: \"2021-09-08 17:01:01.440757\"\n    key-manager:\n      controller_version: 0.5.3.dev12\n      fingerprint: a112a4a7d00c0b5b79e69a2c78c3b50b0caca76a15fe7d79a6ad1305b19ee5ec\n      openstack_version: ussuri\n      state: APPLIED\n      timestamp: \"2021-09-08 17:00:51.822997\"\n    load-balancer:\n      controller_version: 0.5.3.dev12\n      fingerprint: a112a4a7d00c0b5b79e69a2c78c3b50b0caca76a15fe7d79a6ad1305b19ee5ec\n      openstack_version: ussuri\n      state: APPLIED\n      timestamp: \"2021-09-08 17:01:02.462824\"\n    memcached:\n      controller_version: 0.5.3.dev12\n      fingerprint: a112a4a7d00c0b5b79e69a2c78c3b50b0caca76a15fe7d79a6ad1305b19ee5ec\n      openstack_version: ussuri\n      state: APPLIED\n      timestamp: \"2021-09-08 17:01:03.165045\"\n    messaging:\n      controller_version: 0.5.3.dev12\n      fingerprint: a112a4a7d00c0b5b79e69a2c78c3b50b0caca76a15fe7d79a6ad1305b19ee5ec\n      openstack_version: ussuri\n      state: APPLIED\n      timestamp: \"2021-09-08 17:00:58.637506\"\n    networking:\n      controller_version: 0.5.3.dev12\n      fingerprint: a112a4a7d00c0b5b79e69a2c78c3b50b0caca76a15fe7d79a6ad1305b19ee5ec\n      openstack_version: ussuri\n      state: APPLIED\n      timestamp: \"2021-09-08 17:01:35.553483\"\n    object-storage:\n      controller_version: 0.5.3.dev12\n      fingerprint: a112a4a7d00c0b5b79e69a2c78c3b50b0caca76a15fe7d79a6ad1305b19ee5ec\n      openstack_version: ussuri\n      state: APPLIED\n      timestamp: \"2021-09-08 17:01:01.828834\"\n    orchestration:\n      controller_version: 0.5.3.dev12\n      fingerprint: a112a4a7d00c0b5b79e69a2c78c3b50b0caca76a15fe7d79a6ad1305b19ee5ec\n      openstack_version: ussuri\n      state: APPLIED\n      timestamp: \"2021-09-08 17:01:02.846671\"\n    placement:\n      controller_version: 0.5.3.dev12\n      fingerprint: a112a4a7d00c0b5b79e69a2c78c3b50b0caca76a15fe7d79a6ad1305b19ee5ec\n      openstack_version: ussuri\n      state: APPLIED\n      timestamp: \"2021-09-08 17:00:58.039210\"\n    redis:\n      controller_version: 0.5.3.dev12\n      fingerprint: a112a4a7d00c0b5b79e69a2c78c3b50b0caca76a15fe7d79a6ad1305b19ee5ec\n      openstack_version: ussuri\n      state: APPLIED\n      timestamp: \"2021-09-08 17:00:36.562673\"\n</code></pre>"},{"location":"architecture/custom-resources/openstackdeploymentstatus/#health-structure","title":"Health structure","text":"<p>The <code>health</code> subsection provides a brief output on services health of each component</p>"},{"location":"architecture/custom-resources/openstackdeploymentstatus/#osdpl-structure","title":"OsDpl structure","text":"<p>The <code>osdpl</code> subsection describes the overall status of the OpenStack deployment.</p> Element Description <code>cause</code> The cause that triggered the LCM action: <code>update</code> when OsDpl is updated, <code>resume</code> when the OpenStack Controller is restarted <code>changes</code> A string representation of changes in the <code>OpenstackDeployment</code> object <code>controller_version</code> The version of <code>rockoon</code> that handles the LCM action <code>fingerprint</code> The SHA sum of the <code>OpenStackDeployment</code> object spec section <code>openstack_version</code> The current OpenStack version specified in the <code>osdpl</code> object <code>state</code> The current state of the LCM action. - <code>APPLYING</code>: not all operations are completed  - <code>APPLIED</code>: all operations are completed <code>timestamp</code> The timestamp of the status:osdpl section update"},{"location":"architecture/custom-resources/openstackdeploymentstatus/#services-structure","title":"Services structure","text":"<p>The services subsection provides detailed information of LCM performed with a specific service. This is a dictionary where keys are service names, for example, <code>baremetal</code> or <code>compute</code> and values are dictionaries with the following items.</p> <p>Services structure elements</p> Element Description <code>controller_version</code> The version of the <code>rockoon</code> that handles the LCM action on a specific service <code>fingerprint</code> The SHA sum of the <code>OpenStackDeployment</code> object spec section used when performing the LCM on a specific service <code>openstack_version</code> The OpenStack version specified in the <code>osdpl</code> object used when performing the LCM action on a specific service <code>state</code> The current state of the LCM action. - <code>WAITING</code>: waiting for dependencies  - <code>APPLYING</code>: not all operations are completed  - <code>APPLIED</code>: all operations are completed <code>timestamp</code> The timestamp of the status:osdpl section update"},{"location":"architecture/rockoon/configuration/","title":"Configuration","text":"<p>The OpenStack Controller enables you to modify its configuration at runtime without restarting. MOSK stores the controller configuration in the <code>openstack-controller-config</code> <code>ConfigMap</code> in the osh-system namespace of your cluster.</p> <p>To retrieve the OpenStack Controller configuration <code>ConfigMap</code>, run:</p> <pre><code>kubectl get configmaps openstack-controller-config -o yaml\n</code></pre> <p>Example of OpenStackController configuration</p> <pre><code>apiVersion: v1\ndata:\n  extra_conf.ini: |\n    [maintenance]\n    respect_nova_az = false\nkind: ConfigMap\nmetadata:\n  annotations:\n    openstackdeployments.lcm.mirantis.com/skip_update: \"true\"\n  name: openstack-controller-config\n  namespace: osh-system\n</code></pre> <pre><code>[osctl]\n# The number of seconds to wait for all component from application becomes ready\nwait_application_ready_timeout = 1200\n\n# The number of seconds to sleep between checking application ready attempts\nwait_application_ready_delay = 10\n\n# The amount of time to wit for flapping node\nnode_not_ready_flapping_timeout = 120\n\n[helmbundle]\n# The number of seconds to wait for values set in manifest are propagated to child objects.\nmanifest_enable_timeout = 600\n\n# The number of seconds between attempts to check that values were applied.\nmanifest_enable_delay = 10\n\n# The number of seconds to wait for values are removed from manifest and propagated to child objects.\nmanifest_disable_timeout = 600\n\n# The number of seconds between attempts to check that values were removed from release.\nmanifest_disable_delay = 10\n\n# The number of seconds to wait for kubernetes object removal\nmanifest_purge_timeout = 600\n\n# The number of seconds between attempts to check that kubernetes object is removed\nmanifest_purge_delay = 10\n\n# The number of seconds to pause for helmbundle changes\nmanifest_apply_delay = 10\n\n# The number of seconds to run for helm command\nhelm_cmd_timeout = 120\n\n[maintenance]\n# number of instances to migrate concurrently\ninstance_migrate_concurrency = 1\n\n# max number of compute nodes we allow to update in parallel\nnwl_parallel_max_compute = 30\n\n# max number of gateway nodes we allow to update in parallel\nnwl_parallel_max_gateway = 1\n\n# respect nova AZs, when set to true parallel update is allowed only for computes in same AZ\nrespect_nova_az = True\n\n# flag to skip instance check on host before proceeding with node removal. By default is False\n# which means that node removal will be blocked unless at least 1 instance exists on host.\nndr_skip_instance_check = False\n\n# flag to skip volume check on host before proceeding with node removal. By default is False\n# which means that node removal will be blocked unless at least 1 volume exists on host.\n# Volume is tied to specific host only for LVM backend.\nndr_skip_volume_check = False\n</code></pre>"},{"location":"architecture/rockoon/custom-images/","title":"Customize images","text":"<p>OpenStack Controller has default built in images that were verified against different production configurations. However it may be needed to inclide additional patches into openstack code or 3rd party software.</p> <p>OpenStack images are built with help of Loci. Please refer to its documentation to get more detail about build process.</p> <p>To inject a custom image create configmap with <code>&lt;openstackdeployment-name&gt;-artifacts</code> name in <code>openstack</code> namespace and folling data structure:</p> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: osh-dev-artifacts\n  namespace: openstack\ndata:\n  caracal: |\n    libvirt: docker-dev-kaas-virtual.mcp.mirantis.com/general/libvirt:6.0.0-focal-20221028120749\n  antelope: |\n    libvirt: docker-dev-kaas-virtual.mcp.mirantis.com/general/libvirt:6.0.0-focal-20221028120749\n</code></pre>"},{"location":"architecture/rockoon/overview/","title":"OpenStack Controller","text":"<p><code>OpenStack controller</code> is running as a deployment in Kubernetes with multiple subcontrollers that are running as dedicated containers in the deployment. Each subcontroller </p> Container Description <code>osdpl</code> The core subcontroller that handles changes of <code>OpenStackDeployment</code> object <code>secrets</code> Subcontroller that provides data excange between different components <code>health</code> Subcontroller that constantly watching for OpenStack health and reporting its status <code>node</code> Subcontroller that watches for <code>Node</code> object <code>nodemaintenancerequest</code> Subcontroller that provides integration with Kubernetes lifecycle management <code>ceph-secrets</code> Subcontroller that provides integration with <code>Ceph</code> storage <code>osdplstatus</code> Subcontroller responsible for status reporting <code>tf-secrets</code> Subcontroller that provides integration with TungstenFabric"},{"location":"ops/openstack/tempest/","title":"Run tempest tests","text":"<p>The OpenStack Integration Test Suite (Tempest), is a set of integration tests to be run against a live OpenStack environment. This section instructs you on how to verify the workability of your OpenStack deployment using Tempest.</p> <p>To verify an OpenStack deployment using Tempest:</p> <ol> <li>Add <code>tempest</code> to <code>spec:features:services</code> in <code>OpenStackDeployment</code> custom resource.</li> <li>Wait until Tempest is ready. The Tempest tests are launched by the <code>openstack-tempest-run-tests</code> job. To    keep track of the tests execution, run:    <pre><code>kubectl -n openstack logs -l application=tempest,component=run-tests\n</code></pre></li> <li>Get the Tempest results. The Tempest results can be stored in a <code>pvc-tempest</code> PersistentVolumeClaim (PVC).    To get them from a PVC, use:    <pre><code># Run pod and mount pvc to it\ncat &lt;&lt;EOF | kubectl apply -f -\napiVersion: v1\nkind: Pod\nmetadata:\n  name: tempest-test-results-pod\n  namespace: openstack\nspec:\n  nodeSelector:\n    openstack-control-plane: enabled\n  volumes:\n    - name: tempest-pvc-storage\n      persistentVolumeClaim:\n        claimName: pvc-tempest\n  containers:\n    - name: tempest-pvc-container\n      image: ubuntu\n      command: ['sh', '-c', 'sleep infinity']\n      volumeMounts:\n        - mountPath: \"/var/lib/tempest/data\"\n          name: tempest-pvc-storage\nEOF\n</code></pre></li> </ol> <p>To rerun tempest:</p> <ol> <li>Remove <code>tempest</code> from the list of enabled services.</li> <li>Wait until Tempest jobs are removed.</li> <li>Add <code>tempest</code> back to the list of the enabled services.</li> </ol>"},{"location":"ops/openstack/troubleshoot/","title":"Troubleshooting","text":"<p>This section provides the general debugging instructions for your OpenStack on Kubernetes deployment. Start your troubleshooting with the determination of the failing component that can include the Rockoon Operator, Helm, a particular pod or service.</p> <p>Note</p> <p>For Kubernetes cluster debugging and troubleshooting, refer to Kubernetes official documentation: Troubleshoot clusters</p>"},{"location":"ops/openstack/troubleshoot/#debugging-the-helm-releases","title":"Debugging the Helm releases","text":"<ol> <li>Log in to the <code>rockoon</code> pod, where the Helm v3 client is installed, or download the Helm v3 binary locally: <pre><code>kubectl -n osh-system exec -it deployment/rockoon -- bash\n</code></pre></li> <li>Verify the Helm releases statuses: <pre><code>helm3 --namespace openstack list --all\n</code></pre> Example of output: <pre><code>NAME                            NAMESPACE       REVISION        UPDATED                                 STATUS          CHART                           APP VERSION\netcd                            openstack       4               2021-07-09 11:06:25.377538008 +0000 UTC deployed        etcd-0.1.0-mcp-2735\ningress-openstack               openstack       4               2021-07-09 11:06:24.892822083 +0000 UTC deployed        ingress-0.1.0-mcp-2735\nopenstack-barbican              openstack       4               2021-07-09 11:06:25.733684392 +0000 UTC deployed        barbican-0.1.0-mcp-3890\nopenstack-ceph-rgw              openstack       4               2021-07-09 11:06:25.045759981 +0000 UTC deployed        ceph-rgw-0.1.0-mcp-2735\nopenstack-cinder                openstack       4               2021-07-09 11:06:42.702963544 +0000 UTC deployed        cinder-0.1.0-mcp-3890\nopenstack-designate             openstack       4               2021-07-09 11:06:24.400555027 +0000 UTC deployed        designate-0.1.0-mcp-3890\nopenstack-glance                openstack       4               2021-07-09 11:06:25.5916904   +0000 UTC deployed        glance-0.1.0-mcp-3890\nopenstack-heat                  openstack       4               2021-07-09 11:06:25.3998706   +0000 UTC deployed        heat-0.1.0-mcp-3890\nopenstack-horizon               openstack       4               2021-07-09 11:06:23.27538297  +0000 UTC deployed        horizon-0.1.0-mcp-3890\nopenstack-iscsi                 openstack       4               2021-07-09 11:06:37.891858343 +0000 UTC deployed        iscsi-0.1.0-mcp-2735            v1.0.0\nopenstack-keystone              openstack       4               2021-07-09 11:06:24.878052272 +0000 UTC deployed        keystone-0.1.0-mcp-3890\nopenstack-libvirt               openstack       4               2021-07-09 11:06:38.185312907 +0000 UTC deployed        libvirt-0.1.0-mcp-2735\nopenstack-mariadb               openstack       4               2021-07-09 11:06:24.912817378 +0000 UTC deployed        mariadb-0.1.0-mcp-2735\nopenstack-memcached             openstack       4               2021-07-09 11:06:24.852840635 +0000 UTC deployed        memcached-0.1.0-mcp-2735\nopenstack-neutron               openstack       4               2021-07-09 11:06:58.96398517  +0000 UTC deployed        neutron-0.1.0-mcp-3890\nopenstack-neutron-rabbitmq      openstack       4               2021-07-09 11:06:51.454918432 +0000 UTC deployed        rabbitmq-0.1.0-mcp-2735\nopenstack-nova                  openstack       4               2021-07-09 11:06:44.277976646 +0000 UTC deployed        nova-0.1.0-mcp-3890\nopenstack-octavia               openstack       4               2021-07-09 11:06:24.775069513 +0000 UTC deployed        octavia-0.1.0-mcp-3890\nopenstack-openvswitch           openstack       4               2021-07-09 11:06:55.271711021 +0000 UTC deployed        openvswitch-0.1.0-mcp-2735\nopenstack-placement             openstack       4               2021-07-09 11:06:21.954550107 +0000 UTC deployed        placement-0.1.0-mcp-3890\nopenstack-rabbitmq              openstack       4               2021-07-09 11:06:25.431404853 +0000 UTC deployed        rabbitmq-0.1.0-mcp-2735\nopenstack-tempest               openstack       2               2021-07-09 11:06:21.330801212 +0000 UTC deployed        tempest-0.1.0-mcp-3890\n</code></pre></li> </ol>"},{"location":"ops/openstack/troubleshoot/#debugging-the-rockoon-controller","title":"Debugging the Rockoon Controller","text":"<p>The Rockoon Controller is running in several containers in the <code>rockoon-xxxx</code> pod in the <code>osh-system</code> namespace.</p> <p>To verify the status of the Rockoon Controller, run: <pre><code>kubectl -n osh-system get pods\n</code></pre></p> <p>Example of a system response: <pre><code>NAME                                  READY   STATUS    RESTARTS   AGE\nrockoon-5c6947c996-vlrmv            5/5     Running     0          17m\nrockoon-admission-f946dc8d6-6bgn2   1/1     Running     0          4h9m\nrockoon-ensure-resources-5ls8k        0/1     Completed   0          4h12m\n</code></pre></p> <p>To verify the logs for the <code>osdpl</code> container, run: <pre><code>kubectl -n osh-system logs -f &lt;rockoon-xxxx&gt; -c osdpl\n</code></pre></p>"},{"location":"ops/openstack/troubleshoot/#some-pods-are-stuck-in-init","title":"Some pods are stuck in <code>Init</code>","text":"<p>MOSK uses the Kubernetes entrypoint init container to resolve dependencies between objects. If the pod is stuck in Init:0/X, this pod may be waiting for its dependencies.</p> <p>Verify the missing dependencies: <pre><code>kubectl -n openstack logs -f placement-api-84669d79b5-49drw -c init\n</code></pre></p> <p>Example of a system response: <pre><code>Entrypoint WARNING: 2020/04/21 11:52:50 entrypoint.go:72: Resolving dependency Job placement-ks-user in namespace openstack failed: Job Job placement-ks-user in namespace openstack is not completed yet .\nEntrypoint WARNING: 2020/04/21 11:52:52 entrypoint.go:72: Resolving dependency Job placement-ks-endpoints in namespace openstack failed: Job Job placement-ks-endpoints in namespace openstack is not completed yet .\n</code></pre></p>"},{"location":"ops/openstack/upgrade/","title":"Upgrade OpenStack","text":"<p>This section provides instructions on how to upgrade OpenStack to a major version with help of OpenStack Controller.</p> <ol> <li> <p>To start the OpenStack upgrade, change the value of the <code>spec:openstack_version</code> parameter in the <code>OpenStackDeployment</code> object to the target OpenStack release.    After you change the value of the <code>spec:openstack_version</code> parameter, the OpenStack Controller initializes the upgrade process.</p> </li> <li> <p>Verify the upgrade status    <pre><code>kubectl -n openstack get osdplst\n</code></pre>    Example of output    <pre><code>NAME      OPENSTACK VERSION   CONTROLLER VERSION   STATE     LCM PROGRESS   HEALTH   MOSK RELEASE\nosh-dev   antelope             0.17.2.dev250        APPLYING   1/11          13/15 \n</code></pre>    When upgrade finishes, the <code>STATE</code> field should display <code>APPLIED</code>:    <pre><code>kubectl -n openstack get osdplst\nNAME      OPENSTACK VERSION   CONTROLLER VERSION   STATE     LCM PROGRESS   HEALTH   MOSK RELEASE\nosh-dev   caracal             0.17.2.dev250        APPLIED   11/11          15/15\n</code></pre></p> </li> <li> <p>Verify the Upgrade</p> </li> <li>Verify that OpenStack is healthy and operational. All OpenStack components in the <code>health</code> group in the      OpenStackDeploymentStatus CR should be in the <code>Ready</code> state.</li> <li>Verify the workability of your OpenStack deployment by running Tempest against the OpenStack cluster as described in Run Tempest tests.</li> </ol>"},{"location":"quick-start/access-openstack/","title":"Access OpenStack","text":""},{"location":"quick-start/access-openstack/#cli","title":"CLI","text":"<p>You can use the built-in admin CLI client and execute the openstack commands from a dedicated pod deployed in the <code>openstack</code> namespace:</p> <pre><code>kubectl -n openstack exec -it deployment/keystone-client -- bash\n</code></pre> <p>To obtain admin credentials run:</p> <pre><code>kubectl -n openstack exec -it deployment/keystone-client -- bash\ncat /etc/openstack/clouds.yaml\n</code></pre>"},{"location":"quick-start/access-openstack/#horizon","title":"Horizon","text":"<ol> <li>Get IP address of ingress service <pre><code>kubectl -n openstack get svc ingress -o jsonpath='{.status.loadBalancer.ingress[].ip}'\n</code></pre></li> <li>Update local <code>/etc/hosts</code> file to point public domain to ingress external IP <pre><code>10.172.1.100 aodh.it.just.works barbican.it.just.works cinder.it.just.works cloudformation.it.just.works designate.it.just.works glance.it.just.works gnocchi.it.just.works heat.it.just.works horizon.it.just.works keystone.it.just.works metadata.it.just.works neutron.it.just.works nova.it.just.works novncproxy.it.just.works octavia.it.just.works placement.it.just.works spiceproxy.it.just.works\n</code></pre></li> <li>Stup <code>sshuttle</code> to services external IPs <pre><code>sshuttle -r ubuntu@172.16.250.153 10.172.1.0/24\n</code></pre></li> <li>Access to Horizon through web browser https://horizon.it.just.works</li> </ol>"},{"location":"quick-start/aio-installation/","title":"All in One Installation","text":"<p>This paragraph provides a guide how to deploy single node deployment with k0s based Kubernetes cluster and openstack deployed by Rockoon controller.</p>"},{"location":"quick-start/aio-installation/#prepare-vm","title":"Prepare VM","text":"<p>For the deployment we will need Virtual Machine with following minimal requirements.</p> <p>Minimal VM requirements</p> Resource Amount RAM 16Gb CPU 8 DISK 100Gb <p>Supported operation systems</p> <ul> <li>Ubuntu 22.04 (x86_64)</li> </ul>"},{"location":"quick-start/aio-installation/#trigger-deployment","title":"Trigger Deployment","text":"<ol> <li> <p>Download repository with rockoon   <pre><code>git clone https://github.com/Mirantis/rockoon\n</code></pre></p> </li> <li> <p>Trigger deployment   <pre><code>cd rockoon/virtual_lab/\nbash install.sh\n</code></pre></p> </li> </ol>"}]}