{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to Rockoon Controller documentation","text":""},{"location":"#introduction","title":"Introduction","text":"<p>The Rockoon Controller is a Kubernetes operator that implements lifecycle management for OpenStack deployment.</p> <p>The Rockoon is written in Python using Kopf as a Python framework to build Kubernetes operators, and Pykube.</p> <p>The controller subscribes to changes to OpenStackDeployment Kubernetes custom resource and then reacts to these changes by creating, updating, or deleting appropriate resources in Kubernetes.</p>"},{"location":"#getting-help","title":"Getting Help","text":"<ul> <li>File a bug: https://github.com/Mirantis/rockoon/issues</li> <li>Join slack channel</li> </ul>"},{"location":"#developer","title":"Developer","text":"<ul> <li>Contributing: https://TODO</li> <li>Reference Architecture:  https://mirantis.github.io/rockoon</li> </ul>"},{"location":"developer/","title":"Developer Guide","text":""},{"location":"developer/#code-style","title":"Code Style","text":"<p>Rockoon Contoller uses Black code formatter To check your chenages and format them use <pre><code>tox -e black\n</code></pre></p>"},{"location":"developer/#tests","title":"Tests","text":"<p>Each commit should require to pass code styles and unittests. To run unittests locally <pre><code>tox -e py310\n</code></pre></p>"},{"location":"developer/#running-controller-locally","title":"Running controller locally","text":"<p>Rockoon Controller is deployed as helm chart into kubernetes cluster. However there is possibility to run controller locally. For this: <pre><code>tox -e dev\n</code></pre></p>"},{"location":"architecture/rockoon-admission/","title":"OpenStack Controller Admission","text":"<p>The CustomResourceDefinition resource in Kubernetes uses the OpenAPI Specification version 2 to specify the schema of the resource defined. The Kubernetes API outright rejects the resources that do not pass this schema validation.</p> <p>The language of the schema, however, is not expressive enough to define a specific validation logic that may be needed for a given resource. For this purpose, Kubernetes enables the extension of its API with Dynamic Admission Control.</p> <p>For the OpenStackDeployment (OsDpl) CR the ValidatingAdmissionWebhook is a natural choice. It is deployed as part of OpenStack Controller in dedicated deployment by default and performs specific extended validations when an <code>OpenStackDeployment</code> CR is created or updated.</p> <p>The inexhaustive list of additional validations includes:</p> <ul> <li>Deny the OpenStack version downgrade</li> <li>Deny the OpenStack version skip-level upgrade</li> <li>Deny the OpenStack master version deployment</li> <li>Deny upgrade to the OpenStack master version</li> <li>Deny deploying invalid configuration</li> </ul>"},{"location":"architecture/cloud_services/horizon/","title":"Horizon Configuration","text":"<p>This article describes horizon configuration.</p>"},{"location":"architecture/cloud_services/horizon/#custom-themes","title":"Custom Themes","text":"<p>To apply custom horizon theme on your environment use the following snippet for <code>OpenStackDeployment</code> custom resource:</p> <pre><code>spec:\n  features:\n    horizon:\n      themes:\n      - description: my custom theme\n        name: AwesomTheme\n        url: https://url/to/theme.tar.gz\n        sha256summ: \"sha256 of theme archive\"\n        enabled: true\n</code></pre> <p>By default <code>Mirantis</code> theme is enabled, to disabled it use</p> <pre><code>spec:\n  features:\n    horizon:\n      themes:\n      - name: mirantis\n        enabled: false\n</code></pre>"},{"location":"architecture/custom-resources/openstackdeployment/","title":"OpenStackDeployment Custom Resource","text":"<p>Custom Kubernetes resource that describes OpenStack deployment.</p> <pre><code>kubectl get crd openstackdeployments.lcm.mirantis.com -o yaml\n</code></pre> <pre><code>kubectl -n openstack get osdpl -o yaml\n</code></pre> <p>Example of OpenStackDeployment minimal configuration</p> <pre><code>apiVersion: lcm.mirantis.com/v1alpha1\nkind: OpenStackDeployment\nmetadata:\n  annotations:\n  name: osh-dev\n  namespace: openstack\nspec:\n  features:\n    glance:\n      backends:\n        file:\n          pvcstore:\n            default: true\n            pvc:\n              size: 10Gi\n              storage_class_name: lvp-fake-root\n    network_policies:\n      enabled: false\n    neutron:\n      external_networks:\n      - bridge: br-ex\n        interface: veth-phy\n        network_types:\n        - flat\n        physnet: physnet1\n      floating_network:\n        enabled: true\n        physnet: physnet1\n        subnet:\n          gateway: 10.11.12.11\n          pool_end: 10.11.12.200\n          pool_start: 10.11.12.100\n          range: 10.11.12.0/24\n      tunnel_interface: ens3\n    nova:\n      images:\n        backend: local\n      live_migration_interface: ens3\n    services: []\n    ssl:\n      public_endpoints:\n        api_cert:\n          value_from:\n            secret_key_ref:\n              key: api_cert\n              name: osh-dev-hidden\n        api_key:\n          value_from:\n            secret_key_ref:\n              key: api_key\n              name: osh-dev-hidden\n        ca_cert:\n          value_from:\n            secret_key_ref:\n              key: ca_cert\n              name: osh-dev-hidden\n  local_volume_storage_class: lvp-fake-root\n  openstack_version: caracal\n  persistent_volume_storage_class: lvp-fake-root\n  preset: core\n  public_domain_name: it.just.works\n  size: single\n</code></pre>"},{"location":"architecture/custom-resources/openstackdeployment/#main-osdpl-elements","title":"Main osdpl elements","text":"<p>Main elements of OpenStackDeployment custom resource</p> <ul> <li><code>spec.openstack_version</code>: Specifies the OpenStack release to deploy</li> <li><code>spec.preset</code>: String that specifies the name of the preset, a predefined configuration for the OpenStack cluster. A preset includes:<ul> <li>A set of enabled services that includes virtualization, bare metal management, secret management, and others</li> <li>Major features provided by the services, such as VXLAN encapsulation of the tenant traffic</li> </ul> </li> <li><code>spec.size</code>: String that specifies the size category for the OpenStack cluster. The size category defines the internal configuration   of the cluster such as the number of replicas for service workers and timeouts, etc.   The list of supported sizes include:<ul> <li><code>single</code>: single node installation</li> <li><code>tiny</code>: for approximately 10 OpenStack Compute nodes</li> <li><code>small</code>:  for approximately 50 OpenStack Compute nodes</li> <li><code>medium</code>: for approximately 300+ OpenStack Compute nodes  </li> </ul> </li> <li><code>spec.public_domain_name</code>: Specifies the public DNS name for OpenStack services. This is a base DNS name that must be accessible and   resolvable by API clients of your OpenStack cloud. It will be present in the OpenStack endpoints as presented by the OpenStack Identity   service catalog. The TLS certificates used by the OpenStack services (see below) must also be issued to this DNS name. </li> <li><code>spec.features</code>: Contains the top-level collections of settings for the OpenStack deployment that potentially target several OpenStack services.   The section where the customizations should take place.</li> </ul>"},{"location":"architecture/custom-resources/openstackdeployment/#handling-sensitive-information","title":"Handling sensitive information","text":"<p>The <code>OpenStackDeployment</code> custom resource enables you to securely store sensitive fields in Kubernetes secrets. To do that, verify that the reference secret is present in the same namespace as the <code>OpenStackDeployment</code> object and the <code>openstack.lcm.mirantis.com/osdpl_secret</code> label is set to <code>true</code>. The list of fields that can be hidden from <code>OpenStackDeployment</code> is limited and defined by the <code>OpenStackDeployment</code> schema.</p> <p>For example, to hide spec:features:ssl:public_endpoints:api_cert, use the following structure:</p> <pre><code>spec:\n  features:\n    ssl:\n      public_endpoints:\n        api_cert:\n          value_from:\n            secret_key_ref:\n              key: api_cert\n              name: osh-dev-hidden\n</code></pre>"},{"location":"architecture/custom-resources/openstackdeploymentstatus/","title":"OpenStackDeploymentStatus Custom Resource","text":"<p>The resource of kind <code>OpenStackDeploymentStatus</code> is a custom resource that describes the status of an OpenStack deployment. To obtain detailed information about the schema of an OpenStackDeploymentStatus custom resource:</p> <pre><code>kubectl get crd openstackdeploymentstatus.lcm.mirantis.com -o yaml\n</code></pre> <p>To obtain the status definition for a particular OpenStack deployment: <pre><code>kubectl -n openstack get osdplst\n</code></pre></p> <p>Example of system response:</p> <pre><code>NAME      OPENSTACK VERSION   CONTROLLER VERSION   STATE     LCM PROGRESS   HEALTH   MOSK RELEASE\nosh-dev   antelope            0.16.1.dev104        APPLIED   20/20          21/22    MOSK 24.1.3\n</code></pre> <p>Where:</p> <ul> <li><code>OPENSTACK VERSION</code> displays the actual OpenStack version of the deployment</li> <li><code>CONTROLLER VERSION</code> indicates the version of the Rockoon controller responsible for the deployment</li> <li><code>STATE</code> reflects the current status of life-cycle management. The list of possible values includes:</li> <li><code>APPLYING</code> indicates that some Kubernetes objects for applications are in the process of being applied</li> <li><code>APPLIED</code> indicates that all Kubernetes objects for applications have been applied to the latest state</li> <li><code>LCM PROGRESS</code> reflects the current progress of STATE in the format X/Y, where X denotes the number of applications with Kubernetes objects applied and in the actual state, and Y represents the total number of applications managed by the Rockoon controller</li> <li><code>HEALTH</code> provides an overview of the current health status of the OpenStack deployment in the format X/Y, where X represents the number of applications with notReady pods, and Y is the total number of applications managed by the Rockoon controller</li> <li><code>MOSK RELEASE</code> displays the current product release of the OpenStack deployment</li> </ul> <p>Example of <code>OpenStackDeploymentStatus</code></p> <pre><code>kind: OpenStackDeploymentStatus\nmetadata:\n  name: osh-dev\n  namespace: openstack\nspec: {}\nstatus:\n  handle:\n    lastStatus: update\n  health:\n    barbican:\n      api:\n        generation: 2\n        status: Ready\n    cinder:\n      api:\n        generation: 2\n        status: Ready\n      backup:\n        generation: 1\n        status: Ready\n      scheduler:\n        generation: 1\n        status: Ready\n      volume:\n        generation: 1\n        status: Ready\n  osdpl:\n    cause: update\n    changes: '((''add'', (''status'',), None, {''watched'': {''ceph'': {''secret'':\n      {''hash'': ''0fc01c5e2593bc6569562b451b28e300517ec670809f72016ff29b8cbaf3e729''}}}}),)'\n    controller_version: 0.5.3.dev12\n    fingerprint: a112a4a7d00c0b5b79e69a2c78c3b50b0caca76a15fe7d79a6ad1305b19ee5ec\n    openstack_version: ussuri\n    state: APPLIED\n    timestamp: \"2021-09-08 17:01:45.633143\"\n  services:\n    baremetal:\n      controller_version: 0.5.3.dev12\n      fingerprint: a112a4a7d00c0b5b79e69a2c78c3b50b0caca76a15fe7d79a6ad1305b19ee5ec\n      openstack_version: ussuri\n      state: APPLIED\n      timestamp: \"2021-09-08 17:00:54.081353\"\n    block-storage:\n      controller_version: 0.5.3.dev12\n      fingerprint: a112a4a7d00c0b5b79e69a2c78c3b50b0caca76a15fe7d79a6ad1305b19ee5ec\n      openstack_version: ussuri\n      state: APPLIED\n      timestamp: \"2021-09-08 17:00:57.306669\"\n    compute:\n      controller_version: 0.5.3.dev12\n      fingerprint: a112a4a7d00c0b5b79e69a2c78c3b50b0caca76a15fe7d79a6ad1305b19ee5ec\n      openstack_version: ussuri\n      state: APPLIED\n      timestamp: \"2021-09-08 17:01:18.853068\"\n    coordination:\n      controller_version: 0.5.3.dev12\n      fingerprint: a112a4a7d00c0b5b79e69a2c78c3b50b0caca76a15fe7d79a6ad1305b19ee5ec\n      openstack_version: ussuri\n      state: APPLIED\n      timestamp: \"2021-09-08 17:01:00.593719\"\n    dashboard:\n      controller_version: 0.5.3.dev12\n      fingerprint: a112a4a7d00c0b5b79e69a2c78c3b50b0caca76a15fe7d79a6ad1305b19ee5ec\n      openstack_version: ussuri\n      state: APPLIED\n      timestamp: \"2021-09-08 17:00:57.652145\"\n    database:\n      controller_version: 0.5.3.dev12\n      fingerprint: a112a4a7d00c0b5b79e69a2c78c3b50b0caca76a15fe7d79a6ad1305b19ee5ec\n      openstack_version: ussuri\n      state: APPLIED\n      timestamp: \"2021-09-08 17:01:00.233777\"\n    dns:\n      controller_version: 0.5.3.dev12\n      fingerprint: a112a4a7d00c0b5b79e69a2c78c3b50b0caca76a15fe7d79a6ad1305b19ee5ec\n      openstack_version: ussuri\n      state: APPLIED\n      timestamp: \"2021-09-08 17:00:56.540886\"\n    identity:\n      controller_version: 0.5.3.dev12\n      fingerprint: a112a4a7d00c0b5b79e69a2c78c3b50b0caca76a15fe7d79a6ad1305b19ee5ec\n      openstack_version: ussuri\n      state: APPLIED\n      timestamp: \"2021-09-08 17:01:00.961175\"\n    image:\n      controller_version: 0.5.3.dev12\n      fingerprint: a112a4a7d00c0b5b79e69a2c78c3b50b0caca76a15fe7d79a6ad1305b19ee5ec\n      openstack_version: ussuri\n      state: APPLIED\n      timestamp: \"2021-09-08 17:00:58.976976\"\n    ingress:\n      controller_version: 0.5.3.dev12\n      fingerprint: a112a4a7d00c0b5b79e69a2c78c3b50b0caca76a15fe7d79a6ad1305b19ee5ec\n      openstack_version: ussuri\n      state: APPLIED\n      timestamp: \"2021-09-08 17:01:01.440757\"\n    key-manager:\n      controller_version: 0.5.3.dev12\n      fingerprint: a112a4a7d00c0b5b79e69a2c78c3b50b0caca76a15fe7d79a6ad1305b19ee5ec\n      openstack_version: ussuri\n      state: APPLIED\n      timestamp: \"2021-09-08 17:00:51.822997\"\n    load-balancer:\n      controller_version: 0.5.3.dev12\n      fingerprint: a112a4a7d00c0b5b79e69a2c78c3b50b0caca76a15fe7d79a6ad1305b19ee5ec\n      openstack_version: ussuri\n      state: APPLIED\n      timestamp: \"2021-09-08 17:01:02.462824\"\n    memcached:\n      controller_version: 0.5.3.dev12\n      fingerprint: a112a4a7d00c0b5b79e69a2c78c3b50b0caca76a15fe7d79a6ad1305b19ee5ec\n      openstack_version: ussuri\n      state: APPLIED\n      timestamp: \"2021-09-08 17:01:03.165045\"\n    messaging:\n      controller_version: 0.5.3.dev12\n      fingerprint: a112a4a7d00c0b5b79e69a2c78c3b50b0caca76a15fe7d79a6ad1305b19ee5ec\n      openstack_version: ussuri\n      state: APPLIED\n      timestamp: \"2021-09-08 17:00:58.637506\"\n    networking:\n      controller_version: 0.5.3.dev12\n      fingerprint: a112a4a7d00c0b5b79e69a2c78c3b50b0caca76a15fe7d79a6ad1305b19ee5ec\n      openstack_version: ussuri\n      state: APPLIED\n      timestamp: \"2021-09-08 17:01:35.553483\"\n    object-storage:\n      controller_version: 0.5.3.dev12\n      fingerprint: a112a4a7d00c0b5b79e69a2c78c3b50b0caca76a15fe7d79a6ad1305b19ee5ec\n      openstack_version: ussuri\n      state: APPLIED\n      timestamp: \"2021-09-08 17:01:01.828834\"\n    orchestration:\n      controller_version: 0.5.3.dev12\n      fingerprint: a112a4a7d00c0b5b79e69a2c78c3b50b0caca76a15fe7d79a6ad1305b19ee5ec\n      openstack_version: ussuri\n      state: APPLIED\n      timestamp: \"2021-09-08 17:01:02.846671\"\n    placement:\n      controller_version: 0.5.3.dev12\n      fingerprint: a112a4a7d00c0b5b79e69a2c78c3b50b0caca76a15fe7d79a6ad1305b19ee5ec\n      openstack_version: ussuri\n      state: APPLIED\n      timestamp: \"2021-09-08 17:00:58.039210\"\n    redis:\n      controller_version: 0.5.3.dev12\n      fingerprint: a112a4a7d00c0b5b79e69a2c78c3b50b0caca76a15fe7d79a6ad1305b19ee5ec\n      openstack_version: ussuri\n      state: APPLIED\n      timestamp: \"2021-09-08 17:00:36.562673\"\n</code></pre>"},{"location":"architecture/custom-resources/openstackdeploymentstatus/#health-structure","title":"Health structure","text":"<p>The <code>health</code> subsection provides a brief output on services health of each component</p>"},{"location":"architecture/custom-resources/openstackdeploymentstatus/#osdpl-structure","title":"OsDpl structure","text":"<p>The <code>osdpl</code> subsection describes the overall status of the OpenStack deployment.</p> Element Description <code>cause</code> The cause that triggered the LCM action: <code>update</code> when OsDpl is updated, <code>resume</code> when the OpenStack Controller is restarted <code>changes</code> A string representation of changes in the <code>OpenstackDeployment</code> object <code>controller_version</code> The version of <code>rockoon</code> that handles the LCM action <code>fingerprint</code> The SHA sum of the <code>OpenStackDeployment</code> object spec section <code>openstack_version</code> The current OpenStack version specified in the <code>osdpl</code> object <code>state</code> The current state of the LCM action. - <code>APPLYING</code>: not all operations are completed  - <code>APPLIED</code>: all operations are completed <code>timestamp</code> The timestamp of the status:osdpl section update"},{"location":"architecture/custom-resources/openstackdeploymentstatus/#services-structure","title":"Services structure","text":"<p>The services subsection provides detailed information of LCM performed with a specific service. This is a dictionary where keys are service names, for example, <code>baremetal</code> or <code>compute</code> and values are dictionaries with the following items.</p> <p>Services structure elements</p> Element Description <code>controller_version</code> The version of the <code>rockoon</code> that handles the LCM action on a specific service <code>fingerprint</code> The SHA sum of the <code>OpenStackDeployment</code> object spec section used when performing the LCM on a specific service <code>openstack_version</code> The OpenStack version specified in the <code>osdpl</code> object used when performing the LCM action on a specific service <code>state</code> The current state of the LCM action. - <code>WAITING</code>: waiting for dependencies  - <code>APPLYING</code>: not all operations are completed  - <code>APPLIED</code>: all operations are completed <code>timestamp</code> The timestamp of the status:osdpl section update"},{"location":"architecture/rockoon/configuration/","title":"Configuration","text":"<p>The OpenStack Controller enables you to modify its configuration at runtime without restarting. MOSK stores the controller configuration in the <code>openstack-controller-config</code> <code>ConfigMap</code> in the osh-system namespace of your cluster.</p> <p>To retrieve the OpenStack Controller configuration <code>ConfigMap</code>, run:</p> <pre><code>kubectl get configmaps openstack-controller-config -o yaml\n</code></pre> <p>Example of OpenStackController configuration</p> <pre><code>apiVersion: v1\ndata:\n  extra_conf.ini: |\n    [maintenance]\n    respect_nova_az = false\nkind: ConfigMap\nmetadata:\n  annotations:\n    openstackdeployments.lcm.mirantis.com/skip_update: \"true\"\n  name: openstack-controller-config\n  namespace: osh-system\n</code></pre> <pre><code>[osctl]\n# The number of seconds to wait for all component from application becomes ready\nwait_application_ready_timeout = 1200\n\n# The number of seconds to sleep between checking application ready attempts\nwait_application_ready_delay = 10\n\n# The amount of time to wit for flapping node\nnode_not_ready_flapping_timeout = 120\n\n[helmbundle]\n# The number of seconds to wait for values set in manifest are propagated to child objects.\nmanifest_enable_timeout = 600\n\n# The number of seconds between attempts to check that values were applied.\nmanifest_enable_delay = 10\n\n# The number of seconds to wait for values are removed from manifest and propagated to child objects.\nmanifest_disable_timeout = 600\n\n# The number of seconds between attempts to check that values were removed from release.\nmanifest_disable_delay = 10\n\n# The number of seconds to wait for kubernetes object removal\nmanifest_purge_timeout = 600\n\n# The number of seconds between attempts to check that kubernetes object is removed\nmanifest_purge_delay = 10\n\n# The number of seconds to pause for helmbundle changes\nmanifest_apply_delay = 10\n\n# The number of seconds to run for helm command\nhelm_cmd_timeout = 120\n\n[maintenance]\n# number of instances to migrate concurrently\ninstance_migrate_concurrency = 1\n\n# max number of compute nodes we allow to update in parallel\nnwl_parallel_max_compute = 30\n\n# max number of gateway nodes we allow to update in parallel\nnwl_parallel_max_gateway = 1\n\n# respect nova AZs, when set to true parallel update is allowed only for computes in same AZ\nrespect_nova_az = True\n\n# flag to skip instance check on host before proceeding with node removal. By default is False\n# which means that node removal will be blocked unless at least 1 instance exists on host.\nndr_skip_instance_check = False\n\n# flag to skip volume check on host before proceeding with node removal. By default is False\n# which means that node removal will be blocked unless at least 1 volume exists on host.\n# Volume is tied to specific host only for LVM backend.\nndr_skip_volume_check = False\n</code></pre>"},{"location":"architecture/rockoon/custom-images/","title":"Customize images","text":"<p>OpenStack Controller has default built in images that were verified against different production configurations. However it may be needed to inclide additional patches into openstack code or 3rd party software.</p> <p>OpenStack images are built with help of Loci. Please refer to its documentation to get more detail about build process.</p> <p>To inject a custom image create configmap with <code>&lt;openstackdeployment-name&gt;-artifacts</code> name in <code>openstack</code> namespace and folling data structure:</p> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: osh-dev-artifacts\n  namespace: openstack\ndata:\n  caracal: |\n    libvirt: docker-dev-kaas-virtual.mcp.mirantis.com/general/libvirt:6.0.0-focal-20221028120749\n  antelope: |\n    libvirt: docker-dev-kaas-virtual.mcp.mirantis.com/general/libvirt:6.0.0-focal-20221028120749\n</code></pre>"},{"location":"architecture/rockoon/overview/","title":"OpenStack Controller","text":"<p><code>OpenStack controller</code> is running as a deployment in Kubernetes with multiple subcontrollers that are running as dedicated containers in the deployment. Each subcontroller </p> Container Description <code>osdpl</code> The core subcontroller that handles changes of <code>OpenStackDeployment</code> object <code>secrets</code> Subcontroller that provides data excange between different components <code>health</code> Subcontroller that constantly watching for OpenStack health and reporting its status <code>node</code> Subcontroller that watches for <code>Node</code> object <code>nodemaintenancerequest</code> Subcontroller that provides integration with Kubernetes lifecycle management <code>ceph-secrets</code> Subcontroller that provides integration with <code>Ceph</code> storage <code>osdplstatus</code> Subcontroller responsible for status reporting <code>tf-secrets</code> Subcontroller that provides integration with TungstenFabric"},{"location":"ops/openstack/tempest/","title":"Run tempest tests","text":"<p>The OpenStack Integration Test Suite (Tempest), is a set of integration tests to be run against a live OpenStack environment. This section instructs you on how to verify the workability of your OpenStack deployment using Tempest.</p> <p>To verify an OpenStack deployment using Tempest:</p> <ol> <li>Add <code>tempest</code> to <code>spec:features:services</code> in <code>OpenStackDeployment</code> custom resource.</li> <li>Wait until Tempest is ready. The Tempest tests are launched by the <code>openstack-tempest-run-tests</code> job. To    keep track of the tests execution, run:    <pre><code>kubectl -n openstack logs -l application=tempest,component=run-tests\n</code></pre></li> <li>Get the Tempest results. The Tempest results can be stored in a <code>pvc-tempest</code> PersistentVolumeClaim (PVC).    To get them from a PVC, use:    <pre><code># Run pod and mount pvc to it\ncat &lt;&lt;EOF | kubectl apply -f -\napiVersion: v1\nkind: Pod\nmetadata:\n  name: tempest-test-results-pod\n  namespace: openstack\nspec:\n  nodeSelector:\n    openstack-control-plane: enabled\n  volumes:\n    - name: tempest-pvc-storage\n      persistentVolumeClaim:\n        claimName: pvc-tempest\n  containers:\n    - name: tempest-pvc-container\n      image: ubuntu\n      command: ['sh', '-c', 'sleep infinity']\n      volumeMounts:\n        - mountPath: \"/var/lib/tempest/data\"\n          name: tempest-pvc-storage\nEOF\n</code></pre></li> </ol> <p>To rerun tempest:</p> <ol> <li>Remove <code>tempest</code> from the list of enabled services.</li> <li>Wait until Tempest jobs are removed.</li> <li>Add <code>tempest</code> back to the list of the enabled services.</li> </ol>"},{"location":"ops/openstack/troubleshoot/","title":"Troubleshooting","text":"<p>This section provides the general debugging instructions for your OpenStack on Kubernetes deployment. Start your troubleshooting with the determination of the failing component that can include the Rockoon Operator, Helm, a particular pod or service.</p> <p>Note</p> <p>For Kubernetes cluster debugging and troubleshooting, refer to Kubernetes official documentation: Troubleshoot clusters</p>"},{"location":"ops/openstack/troubleshoot/#debugging-the-helm-releases","title":"Debugging the Helm releases","text":"<ol> <li>Log in to the <code>rockoon</code> pod, where the Helm v3 client is installed, or download the Helm v3 binary locally: <pre><code>kubectl -n osh-system exec -it deployment/rockoon -- bash\n</code></pre></li> <li>Verify the Helm releases statuses: <pre><code>helm3 --namespace openstack list --all\n</code></pre> Example of output: <pre><code>NAME                            NAMESPACE       REVISION        UPDATED                                 STATUS          CHART                           APP VERSION\netcd                            openstack       4               2021-07-09 11:06:25.377538008 +0000 UTC deployed        etcd-0.1.0-mcp-2735\ningress-openstack               openstack       4               2021-07-09 11:06:24.892822083 +0000 UTC deployed        ingress-0.1.0-mcp-2735\nopenstack-barbican              openstack       4               2021-07-09 11:06:25.733684392 +0000 UTC deployed        barbican-0.1.0-mcp-3890\nopenstack-ceph-rgw              openstack       4               2021-07-09 11:06:25.045759981 +0000 UTC deployed        ceph-rgw-0.1.0-mcp-2735\nopenstack-cinder                openstack       4               2021-07-09 11:06:42.702963544 +0000 UTC deployed        cinder-0.1.0-mcp-3890\nopenstack-designate             openstack       4               2021-07-09 11:06:24.400555027 +0000 UTC deployed        designate-0.1.0-mcp-3890\nopenstack-glance                openstack       4               2021-07-09 11:06:25.5916904   +0000 UTC deployed        glance-0.1.0-mcp-3890\nopenstack-heat                  openstack       4               2021-07-09 11:06:25.3998706   +0000 UTC deployed        heat-0.1.0-mcp-3890\nopenstack-horizon               openstack       4               2021-07-09 11:06:23.27538297  +0000 UTC deployed        horizon-0.1.0-mcp-3890\nopenstack-iscsi                 openstack       4               2021-07-09 11:06:37.891858343 +0000 UTC deployed        iscsi-0.1.0-mcp-2735            v1.0.0\nopenstack-keystone              openstack       4               2021-07-09 11:06:24.878052272 +0000 UTC deployed        keystone-0.1.0-mcp-3890\nopenstack-libvirt               openstack       4               2021-07-09 11:06:38.185312907 +0000 UTC deployed        libvirt-0.1.0-mcp-2735\nopenstack-mariadb               openstack       4               2021-07-09 11:06:24.912817378 +0000 UTC deployed        mariadb-0.1.0-mcp-2735\nopenstack-memcached             openstack       4               2021-07-09 11:06:24.852840635 +0000 UTC deployed        memcached-0.1.0-mcp-2735\nopenstack-neutron               openstack       4               2021-07-09 11:06:58.96398517  +0000 UTC deployed        neutron-0.1.0-mcp-3890\nopenstack-neutron-rabbitmq      openstack       4               2021-07-09 11:06:51.454918432 +0000 UTC deployed        rabbitmq-0.1.0-mcp-2735\nopenstack-nova                  openstack       4               2021-07-09 11:06:44.277976646 +0000 UTC deployed        nova-0.1.0-mcp-3890\nopenstack-octavia               openstack       4               2021-07-09 11:06:24.775069513 +0000 UTC deployed        octavia-0.1.0-mcp-3890\nopenstack-openvswitch           openstack       4               2021-07-09 11:06:55.271711021 +0000 UTC deployed        openvswitch-0.1.0-mcp-2735\nopenstack-placement             openstack       4               2021-07-09 11:06:21.954550107 +0000 UTC deployed        placement-0.1.0-mcp-3890\nopenstack-rabbitmq              openstack       4               2021-07-09 11:06:25.431404853 +0000 UTC deployed        rabbitmq-0.1.0-mcp-2735\nopenstack-tempest               openstack       2               2021-07-09 11:06:21.330801212 +0000 UTC deployed        tempest-0.1.0-mcp-3890\n</code></pre></li> </ol>"},{"location":"ops/openstack/troubleshoot/#debugging-the-rockoon-controller","title":"Debugging the Rockoon Controller","text":"<p>The Rockoon Controller is running in several containers in the <code>rockoon-xxxx</code> pod in the <code>osh-system</code> namespace.</p> <p>To verify the status of the Rockoon Controller, run: <pre><code>kubectl -n osh-system get pods\n</code></pre></p> <p>Example of a system response: <pre><code>NAME                                  READY   STATUS    RESTARTS   AGE\nrockoon-5c6947c996-vlrmv            5/5     Running     0          17m\nrockoon-admission-f946dc8d6-6bgn2   1/1     Running     0          4h9m\nrockoon-ensure-resources-5ls8k        0/1     Completed   0          4h12m\n</code></pre></p> <p>To verify the logs for the <code>osdpl</code> container, run: <pre><code>kubectl -n osh-system logs -f &lt;rockoon-xxxx&gt; -c osdpl\n</code></pre></p>"},{"location":"ops/openstack/troubleshoot/#some-pods-are-stuck-in-init","title":"Some pods are stuck in <code>Init</code>","text":"<p>MOSK uses the Kubernetes entrypoint init container to resolve dependencies between objects. If the pod is stuck in Init:0/X, this pod may be waiting for its dependencies.</p> <p>Verify the missing dependencies: <pre><code>kubectl -n openstack logs -f placement-api-84669d79b5-49drw -c init\n</code></pre></p> <p>Example of a system response: <pre><code>Entrypoint WARNING: 2020/04/21 11:52:50 entrypoint.go:72: Resolving dependency Job placement-ks-user in namespace openstack failed: Job Job placement-ks-user in namespace openstack is not completed yet .\nEntrypoint WARNING: 2020/04/21 11:52:52 entrypoint.go:72: Resolving dependency Job placement-ks-endpoints in namespace openstack failed: Job Job placement-ks-endpoints in namespace openstack is not completed yet .\n</code></pre></p>"},{"location":"ops/openstack/upgrade/","title":"Upgrade OpenStack","text":"<p>This section provides instructions on how to upgrade OpenStack to a major version with help of OpenStack Controller.</p> <ol> <li> <p>To start the OpenStack upgrade, change the value of the <code>spec:openstack_version</code> parameter in the <code>OpenStackDeployment</code> object to the target OpenStack release.    After you change the value of the <code>spec:openstack_version</code> parameter, the OpenStack Controller initializes the upgrade process.</p> </li> <li> <p>Verify the upgrade status    <pre><code>kubectl -n openstack get osdplst\n</code></pre>    Example of output    <pre><code>NAME      OPENSTACK VERSION   CONTROLLER VERSION   STATE     LCM PROGRESS   HEALTH   MOSK RELEASE\nosh-dev   antelope             0.17.2.dev250        APPLYING   1/11          13/15 \n</code></pre>    When upgrade finishes, the <code>STATE</code> field should display <code>APPLIED</code>:    <pre><code>kubectl -n openstack get osdplst\nNAME      OPENSTACK VERSION   CONTROLLER VERSION   STATE     LCM PROGRESS   HEALTH   MOSK RELEASE\nosh-dev   caracal             0.17.2.dev250        APPLIED   11/11          15/15\n</code></pre></p> </li> <li> <p>Verify the Upgrade</p> </li> <li>Verify that OpenStack is healthy and operational. All OpenStack components in the <code>health</code> group in the      OpenStackDeploymentStatus CR should be in the <code>Ready</code> state.</li> <li>Verify the workability of your OpenStack deployment by running Tempest against the OpenStack cluster as described in Run Tempest tests.</li> </ol>"},{"location":"quick-start/access-openstack/","title":"Access OpenStack","text":""},{"location":"quick-start/access-openstack/#cli","title":"CLI","text":"<p>You can use the built-in admin CLI client and execute the openstack commands from a dedicated pod deployed in the <code>openstack</code> namespace:</p> <pre><code>kubectl -n openstack exec -it deployment/keystone-client -- bash\n</code></pre> <p>To obtain admin credentials run:</p> <pre><code>kubectl -n openstack exec -it deployment/keystone-client -- bash\ncat /etc/openstack/clouds.yaml\n</code></pre>"},{"location":"quick-start/access-openstack/#horizon","title":"Horizon","text":"<ol> <li>Get IP address of ingress service <pre><code>kubectl -n openstack get svc ingress -o jsonpath='{.status.loadBalancer.ingress[].ip}'\n</code></pre></li> <li>Update local <code>/etc/hosts</code> file to point public domain to ingress external IP <pre><code>10.172.1.100 aodh.it.just.works barbican.it.just.works cinder.it.just.works cloudformation.it.just.works designate.it.just.works glance.it.just.works gnocchi.it.just.works heat.it.just.works horizon.it.just.works keystone.it.just.works metadata.it.just.works neutron.it.just.works nova.it.just.works novncproxy.it.just.works octavia.it.just.works placement.it.just.works spiceproxy.it.just.works\n</code></pre></li> <li>Stup <code>sshuttle</code> to services external IPs <pre><code>sshuttle -r ubuntu@172.16.250.153 10.172.1.0/24\n</code></pre></li> <li>Access to Horizon through web browser https://horizon.it.just.works</li> </ol>"},{"location":"quick-start/aio-installation-manual/","title":"All in One Installation","text":"<p>This paragraph provides a guide how to deploy single node deployment with k0s based Kubernetes cluster and openstack deployed by Rockoon controller.</p>"},{"location":"quick-start/aio-installation-manual/#example-command-to-create-an-appropriate-kvm-vm-on-linux-with-virt-manager","title":"Example command to create an appropriate kvm VM on Linux with <code>virt-manager</code>","text":"<p>Needs <code>virt-manager</code>, <code>libvirt</code>, and <code>qemu-kvm</code> installed.</p> <p>Change the path to the public SSH key as you need, the user in the image is <code>ubuntu</code>.</p> <p><pre><code>wget https://cloud-images.ubuntu.com/jammy/current/jammy-server-cloudimg-amd64.img\nqemu-img create -F qcow2 -f qcow2 -b jammy-server-cloudimg-amd64.img rockoon.qcow2 100G\nvirt-install \\\n    --name rockoon \\\n    --import \\\n    --disk path=$PWD/rockoon.qcow2,format=qcow2 \\\n    --vcpus=8 \\\n    --memory=16384 \\\n    --network \"network=default,model=virtio\" \\\n    --osinfo \"ubuntu-22\" \\\n    --arch x86_64 \\\n    --graphics vnc,listen=0.0.0.0 \\\n    --cloud-init clouduser-ssh-key=$HOME/.ssh/id_rsa.pub \\\n    --virt-type kvm \\\n    --watchdog=default \\\n    --noautoconsole\n</code></pre> Once the VM is running, run <code>virsh domifaddr rockoon</code> to find the IP address of the VM to SSH to.</p>"},{"location":"quick-start/aio-installation-manual/#example-command-to-create-an-appropriate-qemu-vm-on-macos-with-lima","title":"Example command to create an appropriate qemu VM on MacOS with <code>lima</code>","text":"<p>As mentioned, only Intel-based Macs are currently supported.</p> <pre><code>brew install lima\nlimactl create \\\n    --name=rockoon \\\n    --tty=false \\\n    --cpus 8 \\\n    --disk 100 \\\n    --memory 16 \\\n    --plain \\\n    --arch x86_64 \\\n    --vm-type qemu \\\n    --set '.cpuType.x86_64 = \"host\"' \\\n    template://ubuntu-22.04\nlimactl start rockoon\nssh -F ~/.lima/rockoon/ssh.config lima-rockoon\n</code></pre>"},{"location":"quick-start/aio-installation-manual/#trigger-deployment","title":"Trigger Deployment","text":"<ol> <li> <p>Download repository with rockoon   <pre><code>git clone https://github.com/Mirantis/rockoon\n</code></pre></p> </li> <li> <p>Trigger deployment   <pre><code>cd rockoon/virtual_lab/\nsudo bash install.sh\n</code></pre></p> </li> </ol>"},{"location":"quick-start/introduction/","title":"Introduction","text":"<p>Rockoon can be deployed in different ways. For a quick start in a preconfigured environment, we have created the TryMOSK image, which allows you to easily launch a virtual machine with Rockoon and additional tools. The process of deploying Rockoon using the TryMOSK image is described in the TryMOSK (Using pre-built image) section.</p> <p>Another option is to deploy Rockoon on your own existing virtual machine. In this case, the sources will be downloaded from GitHub, and some of the Docker images will be built on the virtual machine during the deployment process. This method is described in the Manual Install (Advanced) section.</p>"},{"location":"quick-start/introduction/#host","title":"Host","text":"<p>At the moment hosts with non-x86_64 CPU (like Apple Silicon) are not supported. The required full CPU emulation for virtual machine introduces too much overhead, making the system too slow and unusable.</p>"},{"location":"quick-start/introduction/#prepare-vm","title":"Prepare VM","text":"<p>For the deployment we will need Virtual Machine with following minimal requirements.</p> <p>Minimal VM requirements</p> Resource Amount RAM 16Gb CPU 4 DISK 40Gb <p>Supported operation systems for Manual install (Advanced)</p> <ul> <li>Ubuntu 22.04 (x86_64)</li> </ul>"},{"location":"quick-start/trymosk-installation-aws/","title":"Installation on AWS","text":"<p>You can get a TryMOSK image from the Mirantis CDN server The latest image releases are located in folders with pefix <code>mosk-</code> in the name. You can also use a prepared AMI image on Amazon servers. This article describes the steps required to deploy TryMOSK from a prepared AMI image. At present time they are available on the us-east-2 region.</p>"},{"location":"quick-start/trymosk-installation-aws/#creating-a-trymosk-instance-using-the-amazon-ec2-console","title":"Creating a TryMOSK Instance Using the Amazon EC2 Console","text":"<p>Step 1: Open the Amazon EC2 Console</p> <ol> <li>Sign in to the AWS Management Console.</li> <li>Open the Amazon EC2 console at: https://console.aws.amazon.com/ec2/.</li> </ol> <p>Step 2: Select the AWS Region</p> <ol> <li>In the navigation bar at the top-right, locate the Region selector.</li> <li>Select US East (Ohio).    </li> </ol> <p>Step 3: Create a Security Group</p> <ol> <li>In the left navigation pane, under Network &amp; Security, choose Security Groups.</li> <li>Click Create security group.</li> <li>In the Basic details section, provide:</li> <li>Security group name: A descriptive name (for example: <code>trymosk-sg</code>).</li> <li>Description: Purpose of the security group.</li> <li>In the Inbound rules section:</li> <li>Click Add rule.</li> <li>For Type, choose SSH.</li> <li>If you plan to use OpenVPN, click Add rule again, choose Custom UDP Rule, set Port range to <code>1194</code>, and Protocol to UDP.</li> <li>Click Create security group.    </li> </ol> <p>Step 4: Launch the TryMOSK instance</p> <ol> <li>In the left navigation pane, choose Instances, then click Launch instances.</li> <li>Under Name and tags, set the Name (for example: <code>My TryMOSK</code>).</li> <li>Under Application and OS Images (Amazon Machine Image):</li> <li>Type <code>trymosk</code> in the search box.</li> <li>Select the required image from the results.</li> <li>Under Instance type, choose t2.xlarge (or t2.2xlarge for better performance).</li> <li>Under Key pair (login):</li> <li>Select an existing key pair, or</li> <li>Click Create new key pair to create one.</li> <li>Under Network settings, choose Select existing security group and select the one created in Step 3.</li> <li>Under Configure storage, set Root volume size to 40 GB or more.</li> <li>Click Launch instance.    </li> </ol> <p>Step 5: Get the instance\u2019s Public IP address</p> <ol> <li>Wait until the Instance state changes to Running.</li> <li>Select your instance and note the Public IPv4 address in the details panel.    </li> </ol> <p>Step 6: Connect to the instance via SSH</p> <ul> <li>Linux/macOS:</li> </ul> <pre><code>ssh -i /path/to/private-key.pem ubuntu@&lt;PublicIPv4Address&gt;\n</code></pre> <ul> <li>Windows (PuTTY):</li> <li>Convert your <code>.pem</code> key to <code>.ppk</code> using PuTTYgen.</li> <li>Open PuTTY, enter the public IPv4 address, and load your <code>.ppk</code> key in Connection \u2192 SSH \u2192 Auth.</li> </ul>"},{"location":"quick-start/trymosk-installation-aws/#creating-a-trymosk-instance-using-the-aws-cli","title":"Creating a TryMOSK Instance Using the AWS CLI","text":"<p>This procedure describes how to launch a TryMOSK EC2 instance using the AWS Command Line Interface (AWS CLI).</p> <p>Step 1: Install the AWS CLI</p> <p>Follow the installation instructions in the AWS CLI User Guide:  https://docs.aws.amazon.com/cli/latest/userguide/getting-started-install.html</p> <p>Step 2: Configure the AWS CLI</p> <ol> <li>Follow the quick start configuration guide:     https://docs.aws.amazon.com/cli/latest/userguide/getting-started-quickstart.html</li> <li>Important: Set the default AWS region to us-east-2 to ensure all operations are performed in the correct region:</li> </ol> <pre><code>aws configure set region us-east-2\n</code></pre> <p>Step 3: Create or import a Key Pair</p> <p>You will use this key pair to connect to your EC2 instance via SSH.</p> <p>Option A \u2013 Create a new key pair</p> <pre><code>aws ec2 create-key-pair --key-type rsa --key-name &lt;your-key-name&gt; \\\n--query 'KeyMaterial' --output text &gt; &lt;private-key-file&gt;.pem\n</code></pre> <p>Where:</p> <ul> <li><code>&lt;your-key-name&gt;</code> \u2013 unique key pair name (up to 255 ASCII characters).</li> <li><code>&lt;private-key-file&gt;</code> \u2013 file name for the private RSA key (keep it in a safe place).</li> </ul> <p>Example:</p> <pre><code>aws ec2 create-key-pair --key-type rsa --key-name my-key-for-trymosk \\\n--query 'KeyMaterial' --output text &gt; my_private_key.pem\n</code></pre> <p>Option B \u2013 Import an existing public key</p> <pre><code>aws ec2 import-key-pair --key-name &lt;your-key-name&gt; \\\n--public-key-material fileb://&lt;public-key-file&gt;\n</code></pre> <p>Where:</p> <ul> <li><code>&lt;public-key-file&gt;</code> \u2013 path to the <code>.pub</code> public key file.</li> </ul> <p>Example:</p> <pre><code>aws ec2 import-key-pair --key-name another-key-for-trymosk \\\n--public-key-material fileb://home/ubuntu/test_rsa.pub --output text\n</code></pre> <p>Example response:</p> <pre><code>b8:d6:be:72:9c:60:d8:b8:59:d9:00:ab:bd:9d:8c:f8 another-key-for-trymosk key-00d4cf79ad02dd1c5\n</code></pre> <p>Step 4: Create a Security Group</p> <p>4.1. Find your default VPC ID <pre><code>aws ec2 describe-vpcs --query='Vpcs[?IsDefault].VpcId' --output text\n</code></pre> Example of response: <pre><code>  vpc-4744c62c\n</code></pre></p> <p>4.2. Create the security group <pre><code>aws ec2 create-security-group --group-name &lt;your sg name&gt; \\\n--description \"&lt;your sg description&gt;\" --vpc-id &lt;VPC Id&gt; \\\n--query='GroupId' --output text\n</code></pre> Where:</p> <ul> <li><code>&lt;sg-name&gt;</code> \u2013 security group name (unique within VPC, up to 255 characters, cannot start with <code>sg-</code>).</li> <li><code>&lt;sg-description&gt;</code> \u2013 description of the security group.</li> <li><code>&lt;VPC-ID&gt;</code> \u2013 ID from step 4.1</li> </ul> <p>Example: <pre><code>aws ec2 create-security-group --group-name trymosk-security-group \\\n--description \"Test security group for TryMOSK\" --vpc-id vpc-4744c62c \\\n--query='GroupId' --output text\n</code></pre> Example of response: <pre><code>sg-0d72aa991587b5648\n</code></pre></p> <p>4.3. Add inbound rules Allow SSH (port 22) and OpenVPN (port 1194 UDP): <pre><code>aws ec2 authorize-security-group-ingress --group-id &lt;sg-id&gt; --protocol tcp --port 22 --cidr 0.0.0.0/0\naws ec2 authorize-security-group-ingress --group-id &lt;sg-id&gt; --protocol udp --port 1194 --cidr 0.0.0.0/0\n</code></pre> Where <code>&lt;sg-id&gt;</code> is from step 4.2.</p> <p>Step 5: Find the TryMOSK AMI ID</p> <pre><code>aws ec2 describe-images --filters Name=name,Values=\"trymosk*\" \\\n--query='Images[*].[ImageId,Name,CreationDate]' --output table\n</code></pre> <p>Example response:</p> <pre><code>--------------------------------------------------------------------------------------------------\n|                                         DescribeImages                                         |\n+-----------------------+-------------------------------------------+----------------------------+\n|  ami-0504712c3ecb27331|  trymosk-jammy-amd64-25.1-20250723112122  |  2025-07-29T09:52:53.000Z  |\n+-----------------------+-------------------------------------------+----------------------------+\n</code></pre> <p>Step 6: Launch the TryMOSK instance</p> <pre><code>aws ec2 run-instances --image-id &lt;AMI Id&gt; --count 1 \\\n--instance-type t2.xlarge \\\n--key-name &lt;rsa key pair name&gt; \\\n--security-group-ids &lt;sg id&gt; \\\n--associate-public-ip-address \\\n--block-device-mapping DeviceName=/dev/sda1,Ebs={VolumeSize=&lt;root volume size&gt;} \\\n--query='Instances[0].InstanceId' --output text\n</code></pre> <p>Where:</p> <ul> <li><code>&lt;AMI-ID&gt;</code> \u2013 AMI ID from step 5.</li> <li><code>&lt;rsa-key-name&gt;</code> \u2013 key pair from step 3.</li> <li><code>&lt;sg-id&gt;</code> \u2013 security group from step 4.</li> <li><code>&lt;root-size&gt;</code> \u2013 root volume size (minimum 40 GB).</li> </ul> <p>Example:</p> <pre><code>aws ec2 run-instances --image-id ami-0504712c3ecb27331 --count 1 \\\n--instance-type t2.xlarge \\\n--key-name another-key-for-trymosk \\\n--security-group-ids sg-0d72aa991587b5648 \\\n--associate-public-ip-address \\\n--block-device-mapping DeviceName=/dev/sda1,Ebs={VolumeSize=40} \\\n--query='Instances[0].InstanceId' --output text\n</code></pre> <p>Example of response:</p> <pre><code>i-0c8a0969dfd64c909\n</code></pre> <p>Step 7: Wait for the instance to start</p> <pre><code>aws ec2 describe-instances --instance-ids &lt;your instance Id&gt; \\\n--query='Reservations[0].Instances[0].State.Name' --output text\n</code></pre> <p>Where  <code>&lt;your instance Id&gt;</code> is from step 6. Example:</p> <pre><code>aws ec2 describe-instances --instance-ids i-0c8a0969dfd64c909 \\\n--query='Reservations[0].Instances[0].State.Name' --output text\n</code></pre> <p>Example of response:</p> <pre><code>pending\n</code></pre> <p>Step 8: Get the public IPv4 address</p> <pre><code>aws ec2 describe-instances --instance-ids &lt;your instance Id&gt; \\\n--query='Reservations[0].Instances[0].PublicIpAddress' --output text\n</code></pre> <p>Where <code>&lt;your instance Id&gt;</code> is from step 6. Example:</p> <pre><code>aws ec2 describe-instances --instance-ids i-0c8a0969dfd64c909 \\\n--query='Reservations[0].Instances[0].PublicIpAddress' --output text\n</code></pre> <p>Example of response:</p> <pre><code>18.218.29.107\n</code></pre> <p>Step 9: Connect via SSH</p> <p>From your local computer:</p> <ul> <li>Linux/macOS:</li> </ul> <pre><code>ssh -i /path/to/private-key.pem ubuntu@&lt;PublicIPv4Address&gt;\n</code></pre> <ul> <li>Windows (PuTTY):<ul> <li>Convert your <code>.pem</code> key to <code>.ppk</code> using PuTTYgen.</li> <li>Open PuTTY, enter the public IPv4 address, and load your <code>.ppk</code> key in Connection \u2192 SSH \u2192 Auth.</li> </ul> </li> </ul>"},{"location":"quick-start/trymosk-installation-aws/#setting-up-trymosk-on-an-aws-instance","title":"Setting up TryMOSK on an AWS instance","text":"<p>This section explains how to set up TryMOSK on the EC2 instance created in the previous steps.</p> <p>Step 1: Start the TryMOSK installation</p> <p>The installation process takes approximately 25 minutes. It will:</p> <ul> <li>Install and configure Kubernetes and Rockoon on the instance.</li> <li>Set up the OpenVPN server.</li> <li>Create the OpenVPN client configuration file.</li> </ul> <p>Run the following command to start the setup:</p> <pre><code>sudo screen -d -m /srv/trymosk/launch.sh\n</code></pre> <p>This runs the installation script inside a screen session, allowing it to continue even if your SSH connection is interrupted.</p> <p>Step 2: Monitor the installation</p> <p>You can view the installation log in real time:</p> <pre><code>sudo tail -f /var/log/trymosk-install.log\n</code></pre> <p>Step 3: Access TryMOSK after installation</p> <p>When installation is complete, you can:</p> <ol> <li>Work with MOS directly via the console, or</li> <li>Retrieve the OpenVPN client configuration file and set up a VPN connection from your local computer.</li> </ol> <p>Once connected via VPN, you can use:</p> <ul> <li>Horizon (OpenStack web interface)</li> <li>Mirantis Lens</li> <li>Local <code>kubectl</code> commands to manage TryMOSK services.</li> </ul> <p>Step 4: View system information and credentials</p> <p>After installation, system details, OpenStack version, and admin credentials will be displayed as a Message of the Day (MOTD) upon SSH login.</p> <p>Example output:</p> <pre><code>OS_FAMILY: Debian\nOS_DISTRO: Ubuntu\nOS_DISTRO_VERSION: 22.04\nOPENSTACK_VERSION: caracal\nROCKOON_VERSION: 1.0.11\nADMIN_USERNAME: adminlpsdb63Gvn1\nADMIN_PASSWORD: tLuQfuJVAxxxxxxxxxxxxx\n</code></pre>"}]}